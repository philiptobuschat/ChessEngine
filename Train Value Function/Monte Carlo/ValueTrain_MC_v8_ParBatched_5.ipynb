{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Resources.Model import Model_v8\n",
    "from Resources.Game import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.98\n",
    "value_diff_scale = 50\n",
    "value_diff_scale_early = 1\n",
    "# games saved in batches to reduce i/o stream\n",
    "# each batch is a input file and a label file containing [batch_size] individual games\n",
    "batch_target = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### local counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_wins = 0\n",
    "black_wins = 0\n",
    "draws = 0\n",
    "\n",
    "batch_count = 0         # number of batches locally done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local batches: 0 --  w: 39, b: 48, d: 163\n",
      "local batches: 1 --  w: 80, b: 106, d: 314\n",
      "local batches: 2 --  w: 126, b: 164, d: 460\n",
      "local batches: 3 --  w: 168, b: 215, d: 617\n",
      "local batches: 4 --  w: 218, b: 276, d: 756\n",
      " -- global batches = 1241 --  w: 61958, b: 62212, d: 186990 (total: 311160)\n",
      "local batches: 5 --  w: 282, b: 336, d: 882\n",
      "local batches: 6 --  w: 330, b: 389, d: 1031\n",
      "local batches: 7 --  w: 365, b: 436, d: 1199\n",
      "local batches: 8 --  w: 414, b: 482, d: 1354\n",
      "local batches: 9 --  w: 462, b: 528, d: 1510\n",
      " -- global batches = 1266 --  w: 63308, b: 63446, d: 190656 (total: 317410)\n",
      "local batches: 10 --  w: 523, b: 579, d: 1648\n",
      "local batches: 11 --  w: 583, b: 634, d: 1783\n",
      "local batches: 12 --  w: 663, b: 682, d: 1905\n",
      "local batches: 13 --  w: 734, b: 733, d: 2033\n",
      "local batches: 14 --  w: 793, b: 781, d: 2175\n",
      " -- global batches = 1290 --  w: 64750, b: 64688, d: 193970 (total: 323408)\n",
      "local batches: 15 --  w: 858, b: 842, d: 2298\n",
      "local batches: 16 --  w: 924, b: 884, d: 2439\n",
      "local batches: 17 --  w: 975, b: 933, d: 2588\n",
      "local batches: 18 --  w: 1034, b: 985, d: 2726\n",
      "local batches: 19 --  w: 1099, b: 1036, d: 2859\n",
      " -- global batches = 1316 --  w: 66166, b: 65960, d: 197756 (total: 329882)\n",
      "local batches: 20 --  w: 1167, b: 1083, d: 2993\n",
      "local batches: 21 --  w: 1214, b: 1133, d: 3145\n",
      "local batches: 22 --  w: 1269, b: 1177, d: 3295\n",
      "local batches: 23 --  w: 1317, b: 1229, d: 3444\n",
      "local batches: 24 --  w: 1379, b: 1278, d: 3582\n",
      " -- global batches = 1340 --  w: 67408, b: 67058, d: 201392 (total: 335858)\n",
      "local batches: 25 --  w: 1426, b: 1323, d: 3739\n",
      "local batches: 26 --  w: 1476, b: 1371, d: 3890\n",
      "local batches: 27 --  w: 1532, b: 1421, d: 4033\n",
      "local batches: 28 --  w: 1586, b: 1471, d: 4178\n",
      "local batches: 29 --  w: 1638, b: 1522, d: 4324\n",
      " -- global batches = 1364 --  w: 68612, b: 68182, d: 205040 (total: 341834)\n",
      "local batches: 30 --  w: 1694, b: 1563, d: 4476\n",
      "local batches: 31 --  w: 1752, b: 1606, d: 4624\n",
      "local batches: 32 --  w: 1802, b: 1643, d: 4786\n",
      "local batches: 33 --  w: 1851, b: 1693, d: 4936\n",
      "local batches: 34 --  w: 1898, b: 1740, d: 5091\n",
      " -- global batches = 1389 --  w: 69918, b: 69366, d: 208775 (total: 348059)\n",
      "local batches: 35 --  w: 1948, b: 1790, d: 5240\n",
      "local batches: 36 --  w: 1996, b: 1836, d: 5395\n",
      "local batches: 37 --  w: 2039, b: 1887, d: 5550\n",
      "local batches: 38 --  w: 2090, b: 1936, d: 5699\n",
      "local batches: 39 --  w: 2148, b: 1978, d: 5848\n",
      " -- global batches = 1416 --  w: 71314, b: 70737, d: 212731 (total: 354782)\n",
      "local batches: 40 --  w: 2211, b: 2015, d: 5997\n",
      "local batches: 41 --  w: 2262, b: 2058, d: 6152\n",
      "local batches: 42 --  w: 2320, b: 2110, d: 6291\n",
      "local batches: 43 --  w: 2382, b: 2155, d: 6433\n",
      "local batches: 44 --  w: 2434, b: 2209, d: 6576\n",
      " -- global batches = 1441 --  w: 72568, b: 71992, d: 216447 (total: 361007)\n",
      "local batches: 45 --  w: 2490, b: 2265, d: 6713\n",
      "local batches: 46 --  w: 2542, b: 2307, d: 6868\n",
      "local batches: 47 --  w: 2599, b: 2360, d: 7007\n",
      "local batches: 48 --  w: 2640, b: 2416, d: 7159\n",
      "local batches: 49 --  w: 2688, b: 2461, d: 7315\n",
      " -- global batches = 1466 --  w: 73844, b: 73207, d: 220181 (total: 367232)\n",
      "local batches: 50 --  w: 2742, b: 2504, d: 7467\n",
      "local batches: 51 --  w: 2802, b: 2546, d: 7614\n",
      "local batches: 52 --  w: 2860, b: 2596, d: 7755\n",
      "local batches: 53 --  w: 2908, b: 2639, d: 7913\n",
      "local batches: 54 --  w: 2950, b: 2689, d: 8070\n",
      " -- global batches = 1491 --  w: 75156, b: 74436, d: 223865 (total: 373457)\n",
      "local batches: 55 --  w: 3005, b: 2736, d: 8217\n",
      "local batches: 56 --  w: 3044, b: 2799, d: 8364\n",
      "local batches: 57 --  w: 3107, b: 2854, d: 8495\n",
      "local batches: 58 --  w: 3147, b: 2905, d: 8653\n",
      "local batches: 59 --  w: 3212, b: 2949, d: 8793\n",
      " -- global batches = 1516 --  w: 76523, b: 75743, d: 227416 (total: 379682)\n",
      "local batches: 60 --  w: 3269, b: 3005, d: 8929\n",
      "local batches: 61 --  w: 3322, b: 3061, d: 9069\n",
      "local batches: 62 --  w: 3374, b: 3115, d: 9212\n",
      "local batches: 63 --  w: 3431, b: 3167, d: 9352\n",
      "local batches: 64 --  w: 3489, b: 3219, d: 9491\n",
      " -- global batches = 1540 --  w: 77875, b: 77017, d: 230766 (total: 385658)\n",
      "local batches: 65 --  w: 3545, b: 3268, d: 9634\n",
      "local batches: 66 --  w: 3594, b: 3312, d: 9789\n",
      "local batches: 67 --  w: 3639, b: 3363, d: 9941\n",
      "local batches: 68 --  w: 3691, b: 3414, d: 10087\n",
      "local batches: 69 --  w: 3741, b: 3473, d: 10227\n",
      " -- global batches = 1565 --  w: 79134, b: 78289, d: 234444 (total: 391867)\n",
      "local batches: 70 --  w: 3794, b: 3513, d: 10382\n",
      "local batches: 71 --  w: 3845, b: 3554, d: 10538\n",
      "local batches: 72 --  w: 3881, b: 3617, d: 10687\n",
      "local batches: 73 --  w: 3934, b: 3658, d: 10841\n",
      "local batches: 74 --  w: 3982, b: 3713, d: 10986\n",
      " -- global batches = 1590 --  w: 80330, b: 79547, d: 238190 (total: 398067)\n",
      "local batches: 75 --  w: 4048, b: 3766, d: 11115\n",
      "local batches: 76 --  w: 4094, b: 3815, d: 11268\n",
      "local batches: 77 --  w: 4136, b: 3876, d: 11413\n",
      "local batches: 78 --  w: 4188, b: 3934, d: 11551\n",
      "local batches: 79 --  w: 4226, b: 3988, d: 11707\n",
      " -- global batches = 1615 --  w: 81555, b: 80820, d: 241897 (total: 404272)\n",
      "local batches: 80 --  w: 4276, b: 4048, d: 11845\n",
      "local batches: 81 --  w: 4324, b: 4121, d: 11972\n",
      "local batches: 82 --  w: 4382, b: 4183, d: 12100\n",
      "local batches: 83 --  w: 4436, b: 4254, d: 12223\n",
      "local batches: 84 --  w: 4486, b: 4298, d: 12377\n",
      " -- global batches = 1641 --  w: 82918, b: 82352, d: 245450 (total: 410720)\n",
      "local batches: 85 --  w: 4537, b: 4357, d: 12515\n",
      "local batches: 86 --  w: 4582, b: 4417, d: 12658\n",
      "local batches: 87 --  w: 4632, b: 4479, d: 12794\n",
      "local batches: 88 --  w: 4691, b: 4534, d: 12928\n",
      "local batches: 89 --  w: 4723, b: 4591, d: 13087\n",
      " -- global batches = 1666 --  w: 84188, b: 83805, d: 248927 (total: 416920)\n",
      "local batches: 90 --  w: 4771, b: 4650, d: 13228\n",
      "local batches: 91 --  w: 4812, b: 4700, d: 13385\n",
      "local batches: 92 --  w: 4874, b: 4753, d: 13518\n",
      "local batches: 93 --  w: 4924, b: 4796, d: 13673\n",
      "local batches: 94 --  w: 4982, b: 4848, d: 13811\n",
      " -- global batches = 1692 --  w: 85500, b: 85256, d: 252612 (total: 423368)\n",
      "local batches: 95 --  w: 5036, b: 4912, d: 13940\n",
      "local batches: 96 --  w: 5088, b: 4956, d: 14091\n",
      "local batches: 97 --  w: 5143, b: 5014, d: 14225\n",
      "local batches: 98 --  w: 5190, b: 5070, d: 14369\n",
      "local batches: 99 --  w: 5246, b: 5131, d: 14499\n",
      " -- global batches = 1717 --  w: 86833, b: 86640, d: 256071 (total: 429544)\n",
      "local batches: 100 --  w: 5298, b: 5181, d: 14644\n",
      "local batches: 101 --  w: 5349, b: 5240, d: 14781\n",
      "local batches: 102 --  w: 5393, b: 5288, d: 14936\n",
      "local batches: 103 --  w: 5441, b: 5342, d: 15081\n",
      "local batches: 104 --  w: 5493, b: 5400, d: 15218\n",
      " -- global batches = 1742 --  w: 88140, b: 88009, d: 259570 (total: 435719)\n",
      "local batches: 105 --  w: 5552, b: 5450, d: 15356\n",
      "local batches: 106 --  w: 5589, b: 5512, d: 15504\n",
      "local batches: 107 --  w: 5652, b: 5568, d: 15632\n",
      "local batches: 108 --  w: 5706, b: 5623, d: 15770\n",
      "local batches: 109 --  w: 5754, b: 5689, d: 15903\n",
      " -- global batches = 1767 --  w: 89504, b: 89380, d: 263010 (total: 441894)\n",
      "local batches: 110 --  w: 5802, b: 5740, d: 16051\n",
      "local batches: 111 --  w: 5860, b: 5801, d: 16179\n",
      "local batches: 112 --  w: 5900, b: 5843, d: 16343\n",
      "local batches: 113 --  w: 5944, b: 5891, d: 16497\n",
      "local batches: 114 --  w: 6001, b: 5945, d: 16632\n",
      " -- global batches = 1791 --  w: 90651, b: 90773, d: 266383 (total: 447807)\n",
      "local batches: 115 --  w: 6056, b: 6005, d: 16763\n",
      "local batches: 116 --  w: 6112, b: 6050, d: 16908\n",
      "local batches: 117 --  w: 6163, b: 6102, d: 17051\n",
      "local batches: 118 --  w: 6217, b: 6161, d: 17184\n",
      "local batches: 119 --  w: 6279, b: 6207, d: 17322\n",
      " -- global batches = 1816 --  w: 92004, b: 92175, d: 269778 (total: 453957)\n",
      "local batches: 120 --  w: 6336, b: 6253, d: 17465\n",
      "local batches: 121 --  w: 6393, b: 6296, d: 17611\n",
      "local batches: 122 --  w: 6447, b: 6360, d: 17739\n",
      "local batches: 123 --  w: 6510, b: 6405, d: 17877\n",
      "local batches: 124 --  w: 6566, b: 6464, d: 18008\n",
      " -- global batches = 1841 --  w: 93409, b: 93614, d: 273084 (total: 460107)\n",
      "local batches: 125 --  w: 6623, b: 6521, d: 18140\n",
      "local batches: 126 --  w: 6689, b: 6571, d: 18269\n",
      "local batches: 127 --  w: 6744, b: 6621, d: 18409\n",
      "local batches: 128 --  w: 6806, b: 6676, d: 18537\n",
      "local batches: 129 --  w: 6868, b: 6717, d: 18679\n",
      " -- global batches = 1866 --  w: 94769, b: 94962, d: 276507 (total: 466238)\n",
      "local batches: 130 --  w: 6914, b: 6773, d: 18822\n",
      "local batches: 131 --  w: 6966, b: 6833, d: 18955\n",
      "local batches: 132 --  w: 7015, b: 6897, d: 19087\n",
      "local batches: 133 --  w: 7066, b: 6941, d: 19237\n",
      "local batches: 134 --  w: 7124, b: 6977, d: 19388\n",
      " -- global batches = 1893 --  w: 96271, b: 96371, d: 280211 (total: 472853)\n",
      "local batches: 135 --  w: 7195, b: 7030, d: 19509\n",
      "local batches: 136 --  w: 7258, b: 7070, d: 19651\n",
      "local batches: 137 --  w: 7299, b: 7131, d: 19794\n",
      "local batches: 138 --  w: 7352, b: 7191, d: 19926\n",
      "local batches: 139 --  w: 7407, b: 7240, d: 20067\n",
      " -- global batches = 1918 --  w: 97569, b: 97733, d: 283676 (total: 478978)\n",
      "local batches: 140 --  w: 7449, b: 7301, d: 20209\n",
      "local batches: 141 --  w: 7508, b: 7369, d: 20327\n",
      "local batches: 142 --  w: 7563, b: 7422, d: 20464\n",
      "local batches: 143 --  w: 7624, b: 7471, d: 20599\n",
      "local batches: 144 --  w: 7668, b: 7526, d: 20744\n",
      " -- global batches = 1943 --  w: 98985, b: 99072, d: 287045 (total: 485102)\n",
      "local batches: 145 --  w: 7725, b: 7577, d: 20880\n",
      "local batches: 146 --  w: 7782, b: 7634, d: 21010\n",
      "local batches: 147 --  w: 7844, b: 7697, d: 21129\n",
      "local batches: 148 --  w: 7906, b: 7744, d: 21264\n",
      "local batches: 149 --  w: 7966, b: 7799, d: 21393\n",
      " -- global batches = 1968 --  w: 100462, b: 100436, d: 290304 (total: 491202)\n",
      "local batches: 150 --  w: 8024, b: 7870, d: 21508\n",
      "local batches: 151 --  w: 8084, b: 7926, d: 21636\n",
      "local batches: 152 --  w: 8135, b: 7992, d: 21763\n",
      "local batches: 153 --  w: 8201, b: 8060, d: 21873\n",
      "local batches: 154 --  w: 8260, b: 8121, d: 21997\n",
      " -- global batches = 1993 --  w: 101890, b: 101924, d: 293488 (total: 497302)\n",
      "local batches: 155 --  w: 8324, b: 8182, d: 22115\n",
      "local batches: 156 --  w: 8391, b: 8237, d: 22236\n",
      "local batches: 157 --  w: 8444, b: 8296, d: 22367\n",
      "local batches: 158 --  w: 8509, b: 8351, d: 22490\n",
      "local batches: 159 --  w: 8558, b: 8402, d: 22633\n",
      " -- global batches = 2018 --  w: 103359, b: 103278, d: 296744 (total: 503381)\n",
      "local batches: 160 --  w: 8617, b: 8456, d: 22763\n",
      "local batches: 161 --  w: 8678, b: 8507, d: 22894\n",
      "local batches: 162 --  w: 8736, b: 8562, d: 23024\n",
      "local batches: 163 --  w: 8797, b: 8619, d: 23149\n",
      "local batches: 164 --  w: 8849, b: 8671, d: 23288\n",
      " -- global batches = 2043 --  w: 104768, b: 104641, d: 300047 (total: 509456)\n",
      "local batches: 165 --  w: 8898, b: 8733, d: 23420\n",
      "local batches: 166 --  w: 8959, b: 8801, d: 23534\n",
      "local batches: 167 --  w: 9003, b: 8851, d: 23683\n",
      "local batches: 168 --  w: 9052, b: 8902, d: 23826\n",
      "local batches: 169 --  w: 9122, b: 8947, d: 23954\n",
      " -- global batches = 2068 --  w: 106120, b: 105947, d: 303464 (total: 515531)\n",
      "local batches: 170 --  w: 9176, b: 9005, d: 24085\n",
      "local batches: 171 --  w: 9245, b: 9062, d: 24202\n",
      "local batches: 172 --  w: 9312, b: 9119, d: 24320\n",
      "local batches: 173 --  w: 9371, b: 9170, d: 24452\n",
      "local batches: 174 --  w: 9411, b: 9221, d: 24603\n",
      " -- global batches = 2094 --  w: 107633, b: 107395, d: 306805 (total: 521833)\n",
      "local batches: 175 --  w: 9468, b: 9283, d: 24726\n",
      "local batches: 176 --  w: 9530, b: 9325, d: 24864\n",
      "local batches: 177 --  w: 9584, b: 9374, d: 25003\n",
      "local batches: 178 --  w: 9644, b: 9426, d: 25133\n",
      "local batches: 179 --  w: 9696, b: 9483, d: 25266\n",
      " -- global batches = 2119 --  w: 109005, b: 108754, d: 310124 (total: 527883)\n",
      "local batches: 180 --  w: 9756, b: 9535, d: 25396\n",
      "local batches: 181 --  w: 9817, b: 9579, d: 25533\n",
      "local batches: 182 --  w: 9876, b: 9627, d: 25668\n",
      "local batches: 183 --  w: 9930, b: 9672, d: 25811\n",
      "local batches: 184 --  w: 9977, b: 9727, d: 25951\n",
      " -- global batches = 2144 --  w: 110343, b: 110055, d: 313535 (total: 533933)\n",
      "local batches: 185 --  w: 10036, b: 9788, d: 26073\n",
      "local batches: 186 --  w: 10105, b: 9826, d: 26208\n",
      "local batches: 187 --  w: 10157, b: 9887, d: 26337\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/Philip/Desktop/GitHub/ChessEngine/Train Value Function/Monte Carlo/ValueTrain_MC_v8_ParBatched_5.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v8_ParBatched_5.ipynb#W5sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m board_batch\u001b[39m.\u001b[39mappend(board_to_tensor(game\u001b[39m.\u001b[39mpieces))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v8_ParBatched_5.ipynb#W5sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m game\u001b[39m.\u001b[39mFlipBoard()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v8_ParBatched_5.ipynb#W5sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mif\u001b[39;00m game\u001b[39m.\u001b[39;49mis_over():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v8_ParBatched_5.ipynb#W5sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     mate \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v8_ParBatched_5.ipynb#W5sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GitHub/ChessEngine/Resources/Game.py:100\u001b[0m, in \u001b[0;36mGame.is_over\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m (tot \u001b[39m==\u001b[39m \u001b[39m22\u001b[39m\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m piece_counts[\u001b[39m2\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mor\u001b[39;00m (tot \u001b[39m==\u001b[39m \u001b[39m22\u001b[39m\u001b[39m+\u001b[39m\u001b[39m12\u001b[39m \u001b[39mand\u001b[39;00m piece_counts[\u001b[39m12\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     98\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m moves \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPossibleMoves()\n\u001b[1;32m    102\u001b[0m \u001b[39m# No moves available -> stalemate or checkmate\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(moves) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \n",
      "File \u001b[0;32m~/Desktop/GitHub/ChessEngine/Resources/Game.py:755\u001b[0m, in \u001b[0;36mGame.PossibleMoves\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpieces[y, x] \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[1;32m    754\u001b[0m     queens\u001b[39m.\u001b[39mappend((y, x))\n\u001b[0;32m--> 755\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpieces[y, x] \u001b[39m==\u001b[39m \u001b[39m6\u001b[39m:\n\u001b[1;32m    756\u001b[0m     king \u001b[39m=\u001b[39m (y, x)\n\u001b[1;32m    757\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpieces[y, x] \u001b[39m==\u001b[39m \u001b[39m11\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# keep generating new batches of data until stopped\n",
    "while True:\n",
    "\n",
    "    batch_white_wins = 0\n",
    "    batch_black_wins = 0\n",
    "    batch_draws = 0\n",
    "\n",
    "    # load newest model\n",
    "    model = Model_v8()\n",
    "    model_saves = os.listdir('../Monte Carlo/Model Saves MC v8_3')\n",
    "    if len(model_saves) > 0:\n",
    "        newest_model = max(int(i[6:-8]) for i in model_saves)\n",
    "        model.load_state_dict(torch.load('../Monte Carlo/Model Saves MC v8_3/model_{}_batches'.format(newest_model)))\n",
    "    else:\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    # print('loaded model ', newest_model)\n",
    "\n",
    "    with open('/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v8_3/stats', 'rb') as f:\n",
    "        stats = torch.load(f)\n",
    "    stats = stats.int()\n",
    "    global_white_wins   = stats[1]\n",
    "    global_black_wins   = stats[2]\n",
    "    global_draws        = stats[3]\n",
    "\n",
    "    percentage_decisive = (global_white_wins + global_black_wins) / (global_white_wins + global_black_wins + global_draws)\n",
    "    batch_size = int(batch_target // percentage_decisive.item()) # so that on average we have [batch_target] decisive games\n",
    "\n",
    "    meta_games = []\n",
    "    meta_boards_white = []; meta_boards_black = []\n",
    "    i = 0\n",
    "\n",
    "    meta_active = []\n",
    "\n",
    "    for game_ind in range(batch_size):\n",
    "        meta_games.append( Game() )\n",
    "        meta_boards_white.append([]); meta_boards_black.append([])\n",
    "        meta_active.append(True)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # print('start with {} games'.format(batch_size))\n",
    "\n",
    "    while True in meta_active:\n",
    "\n",
    "        # if i % 20 == 0:\n",
    "        #     print('i = {}, with {} active games '.format(i, meta_active.count(True)))\n",
    "\n",
    "        i += 1\n",
    "        full_board_batch = []\n",
    "        meta_board_batch_sizes = [] # save batch sizes to split model output afterwards\n",
    "        meta_moves = []\n",
    "\n",
    "        # go through games, collect positions for value evaluations\n",
    "        for g, game in enumerate(meta_games):\n",
    "\n",
    "            if not meta_active[g]:\n",
    "                meta_board_batch_sizes.append(0)\n",
    "                meta_moves.append([])\n",
    "                continue\n",
    "\n",
    "            meta_moves.append(game.PossibleMoves())\n",
    "            game_ini = game.copy()\n",
    "            board_batch = [board_to_tensor(game.pieces)]\n",
    "            mate = False\n",
    "\n",
    "            for move in meta_moves[-1]:\n",
    "                game.PlayMove(move)\n",
    "                board_batch.append(board_to_tensor(game.pieces))\n",
    "                game.FlipBoard()\n",
    "                if game.is_over():\n",
    "                    mate = True\n",
    "                    break\n",
    "                game = game_ini.copy()\n",
    "\n",
    "            meta_games[g] = game.copy()\n",
    "\n",
    "            if mate:\n",
    "                meta_active[g] = False\n",
    "                meta_board_batch_sizes.append(0)\n",
    "                game.FlipBoard()\n",
    "                if i % 2 == 1:\n",
    "                    meta_boards_white[g].append(board_to_bool_tensor(game.pieces))\n",
    "                if i % 2 == 0:\n",
    "                    meta_boards_black[g].append(board_to_bool_tensor(game.pieces))\n",
    "\n",
    "            if not mate:\n",
    "                full_board_batch = full_board_batch + board_batch\n",
    "                meta_board_batch_sizes.append(len(board_batch))\n",
    "\n",
    "        if len(full_board_batch) == 0:\n",
    "            break\n",
    "\n",
    "        # get values of all positions\n",
    "        full_board_batch = torch.stack(full_board_batch)\n",
    "        out = model(full_board_batch).detach()\n",
    "        meta_values = torch.split(out, meta_board_batch_sizes)\n",
    "\n",
    "        # make moves for all games\n",
    "        for g, game in enumerate(meta_games):\n",
    "\n",
    "            if not meta_active[g]:\n",
    "                continue\n",
    "            values = meta_values[g]\n",
    "            if i < 7:\n",
    "                scale = value_diff_scale_early\n",
    "            else:\n",
    "                scale = value_diff_scale\n",
    "            values_diff = [scale*(values[i] - values[0]) for i in range(1, len(values))]\n",
    "            move_prob = torch.softmax(torch.Tensor(values_diff), dim=0).numpy()\n",
    "            chosen_i = np.random.choice(range(len(meta_moves[g])), p=move_prob)\n",
    "            chosen_move = meta_moves[g][chosen_i]\n",
    "            game.PlayMove(chosen_move)\n",
    "            if i % 2 == 1:\n",
    "                meta_boards_white[g].append(board_to_bool_tensor(game.pieces))\n",
    "            if i % 2 == 0:\n",
    "                meta_boards_black[g].append(board_to_bool_tensor(game.pieces))\n",
    "            game.FlipBoard()\n",
    "\n",
    "    # print('games done, start evaluating')\n",
    "\n",
    "    meta_inputs = []\n",
    "    meta_labels = []\n",
    "\n",
    "    for g, game in enumerate(meta_games):\n",
    "        \n",
    "        winner = game.get_winner()\n",
    "        if winner == 'draw':\n",
    "            draws += 1; batch_draws += 1\n",
    "            continue\n",
    "            reward_white = 0;   reward_black = 0\n",
    "\n",
    "        elif winner == 'white':\n",
    "            white_wins += 1; batch_white_wins += 1\n",
    "            reward_white = 1;   reward_black = -1\n",
    "\n",
    "        elif winner == 'black':\n",
    "            black_wins += 1; batch_black_wins += 1\n",
    "            reward_white = -1;  reward_black = 1\n",
    "\n",
    "        labels_white = [reward_white * gamma**(len(meta_boards_white[g]) - 1 - i) for i in range(len(meta_boards_white[g]))]\n",
    "        labels_black = [reward_black * gamma**(len(meta_boards_black[g]) - 1 - i) for i in range(len(meta_boards_black[g]))]\n",
    "\n",
    "        meta_inputs = meta_inputs + meta_boards_white[g] + meta_boards_black[g]\n",
    "        meta_labels = meta_labels + labels_white + labels_black\n",
    "\n",
    "    if batch_white_wins + batch_black_wins == 0:\n",
    "        print('no decisive games in the whole batch -> skip to next batch (batch size too small?)')\n",
    "        continue\n",
    "\n",
    "    inputs_tens = torch.stack(meta_inputs)\n",
    "    labels_tens = torch.Tensor(meta_labels)\n",
    "\n",
    "    # print('evaluation done, save batch')\n",
    "\n",
    "    with open('/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v8_3/stats', 'rb') as f:\n",
    "        stats = torch.load(f)\n",
    "    stats = stats.int()\n",
    "    stats[0] += 1;                  stats[1] += batch_white_wins\n",
    "    stats[2] += batch_black_wins;   stats[3] += batch_draws\n",
    "    torch.save(stats, '/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v8_3/stats')\n",
    "\n",
    "    # print('batch index = ', stats[0])\n",
    "\n",
    "    new_batch_index = stats[0]\n",
    "    torch.save(inputs_tens, '/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v8_3/inputs_{}'.format(new_batch_index))\n",
    "    torch.save(labels_tens, '/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v8_3/labels_{}'.format(new_batch_index))\n",
    "\n",
    "    print('local batches: {} --  w: {}, b: {}, d: {}'.format(batch_count, white_wins, black_wins, draws))\n",
    "\n",
    "    batch_count += 1\n",
    "\n",
    "    if batch_count % 5 == 0:\n",
    "        print(' -- global batches = {} --  w: {}, b: {}, d: {} (total: {})'.format(\n",
    "            new_batch_index, stats[1], stats[2], stats[3], stats[1] + stats[2] + stats[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
