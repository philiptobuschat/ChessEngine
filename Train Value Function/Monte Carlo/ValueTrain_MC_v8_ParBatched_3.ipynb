{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Resources.Model import Model_v8\n",
    "from Resources.Game import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.98\n",
    "value_diff_scale = 50\n",
    "value_diff_scale_early = 1\n",
    "# games saved in batches to reduce i/o stream\n",
    "# each batch is a input file and a label file containing [batch_size] individual games\n",
    "batch_target = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### local counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_wins = 0\n",
    "black_wins = 0\n",
    "draws = 0\n",
    "\n",
    "batch_count = 0         # number of batches locally done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local batches: 0 --  w: 49, b: 41, d: 160\n",
      "local batches: 1 --  w: 95, b: 90, d: 315\n",
      "local batches: 2 --  w: 149, b: 137, d: 464\n",
      "local batches: 3 --  w: 211, b: 183, d: 606\n",
      "local batches: 4 --  w: 274, b: 256, d: 720\n",
      " -- global batches = 1239 --  w: 61851, b: 62101, d: 186708 (total: 310660)\n",
      "local batches: 5 --  w: 331, b: 308, d: 861\n",
      "local batches: 6 --  w: 386, b: 361, d: 1003\n",
      "local batches: 7 --  w: 431, b: 408, d: 1161\n",
      "local batches: 8 --  w: 489, b: 455, d: 1306\n",
      "local batches: 9 --  w: 546, b: 492, d: 1462\n",
      " -- global batches = 1263 --  w: 63147, b: 63313, d: 190200 (total: 316660)\n",
      "local batches: 10 --  w: 599, b: 550, d: 1601\n",
      "local batches: 11 --  w: 657, b: 608, d: 1735\n",
      "local batches: 12 --  w: 725, b: 667, d: 1858\n",
      "local batches: 13 --  w: 795, b: 720, d: 1985\n",
      "local batches: 14 --  w: 852, b: 771, d: 2127\n",
      " -- global batches = 1287 --  w: 64577, b: 64542, d: 193541 (total: 322660)\n",
      "local batches: 15 --  w: 898, b: 830, d: 2271\n",
      "local batches: 16 --  w: 943, b: 875, d: 2430\n",
      "local batches: 17 --  w: 990, b: 914, d: 2593\n",
      "local batches: 18 --  w: 1048, b: 964, d: 2734\n",
      "local batches: 19 --  w: 1105, b: 1011, d: 2879\n",
      " -- global batches = 1312 --  w: 65924, b: 65761, d: 197201 (total: 328886)\n",
      "local batches: 20 --  w: 1154, b: 1058, d: 3032\n",
      "local batches: 21 --  w: 1202, b: 1105, d: 3186\n",
      "local batches: 22 --  w: 1261, b: 1146, d: 3335\n",
      "local batches: 23 --  w: 1303, b: 1198, d: 3490\n",
      "local batches: 24 --  w: 1354, b: 1238, d: 3648\n",
      " -- global batches = 1337 --  w: 67230, b: 66937, d: 200944 (total: 335111)\n",
      "local batches: 25 --  w: 1405, b: 1287, d: 3797\n",
      "local batches: 26 --  w: 1454, b: 1322, d: 3962\n",
      "local batches: 27 --  w: 1498, b: 1357, d: 4132\n",
      "local batches: 28 --  w: 1555, b: 1405, d: 4276\n",
      "local batches: 29 --  w: 1617, b: 1447, d: 4421\n",
      " -- global batches = 1362 --  w: 68500, b: 68072, d: 204764 (total: 341336)\n",
      "local batches: 30 --  w: 1673, b: 1491, d: 4570\n",
      "local batches: 31 --  w: 1726, b: 1530, d: 4727\n",
      "local batches: 32 --  w: 1783, b: 1585, d: 4864\n",
      "local batches: 33 --  w: 1826, b: 1635, d: 5020\n",
      "local batches: 34 --  w: 1885, b: 1711, d: 5134\n",
      " -- global batches = 1387 --  w: 69821, b: 69273, d: 208467 (total: 347561)\n",
      "local batches: 35 --  w: 1936, b: 1765, d: 5278\n",
      "local batches: 36 --  w: 1982, b: 1813, d: 5433\n",
      "local batches: 37 --  w: 2029, b: 1860, d: 5588\n",
      "local batches: 38 --  w: 2089, b: 1914, d: 5723\n",
      "local batches: 39 --  w: 2137, b: 1974, d: 5864\n",
      " -- global batches = 1412 --  w: 71080, b: 70544, d: 212162 (total: 353786)\n",
      "local batches: 40 --  w: 2183, b: 2018, d: 6023\n",
      "local batches: 41 --  w: 2222, b: 2063, d: 6188\n",
      "local batches: 42 --  w: 2273, b: 2112, d: 6337\n",
      "local batches: 43 --  w: 2313, b: 2184, d: 6474\n",
      "local batches: 44 --  w: 2366, b: 2227, d: 6627\n",
      " -- global batches = 1437 --  w: 72369, b: 71772, d: 215870 (total: 360011)\n",
      "local batches: 45 --  w: 2421, b: 2276, d: 6772\n",
      "local batches: 46 --  w: 2483, b: 2324, d: 6911\n",
      "local batches: 47 --  w: 2527, b: 2375, d: 7065\n",
      "local batches: 48 --  w: 2571, b: 2427, d: 7218\n",
      "local batches: 49 --  w: 2619, b: 2478, d: 7368\n",
      " -- global batches = 1462 --  w: 73635, b: 73030, d: 219571 (total: 366236)\n",
      "local batches: 50 --  w: 2672, b: 2526, d: 7516\n",
      "local batches: 51 --  w: 2726, b: 2575, d: 7662\n",
      "local batches: 52 --  w: 2775, b: 2630, d: 7807\n",
      "local batches: 53 --  w: 2829, b: 2685, d: 7947\n",
      "local batches: 54 --  w: 2871, b: 2738, d: 8101\n",
      " -- global batches = 1487 --  w: 74968, b: 74255, d: 223238 (total: 372461)\n",
      "local batches: 55 --  w: 2924, b: 2778, d: 8257\n",
      "local batches: 56 --  w: 2978, b: 2836, d: 8394\n",
      "local batches: 57 --  w: 3034, b: 2890, d: 8533\n",
      "local batches: 58 --  w: 3088, b: 2945, d: 8673\n",
      "local batches: 59 --  w: 3152, b: 3003, d: 8800\n",
      " -- global batches = 1512 --  w: 76315, b: 75551, d: 226820 (total: 378686)\n",
      "local batches: 60 --  w: 3202, b: 3064, d: 8938\n",
      "local batches: 61 --  w: 3264, b: 3111, d: 9078\n",
      "local batches: 62 --  w: 3325, b: 3163, d: 9214\n",
      "local batches: 63 --  w: 3374, b: 3217, d: 9360\n",
      "local batches: 64 --  w: 3423, b: 3272, d: 9505\n",
      " -- global batches = 1537 --  w: 77708, b: 76875, d: 230328 (total: 384911)\n",
      "local batches: 65 --  w: 3474, b: 3327, d: 9647\n",
      "local batches: 66 --  w: 3525, b: 3373, d: 9798\n",
      "local batches: 67 --  w: 3569, b: 3422, d: 9953\n",
      "local batches: 68 --  w: 3624, b: 3470, d: 10098\n",
      "local batches: 69 --  w: 3665, b: 3529, d: 10247\n",
      " -- global batches = 1562 --  w: 78965, b: 78120, d: 234035 (total: 391120)\n",
      "local batches: 70 --  w: 3717, b: 3585, d: 10387\n",
      "local batches: 71 --  w: 3759, b: 3643, d: 10535\n",
      "local batches: 72 --  w: 3806, b: 3692, d: 10687\n",
      "local batches: 73 --  w: 3860, b: 3733, d: 10840\n",
      "local batches: 74 --  w: 3917, b: 3773, d: 10991\n",
      " -- global batches = 1587 --  w: 80189, b: 79404, d: 237730 (total: 397323)\n",
      "local batches: 75 --  w: 3965, b: 3819, d: 11145\n",
      "local batches: 76 --  w: 4019, b: 3878, d: 11281\n",
      "local batches: 77 --  w: 4068, b: 3928, d: 11430\n",
      "local batches: 78 --  w: 4119, b: 3985, d: 11571\n",
      "local batches: 79 --  w: 4168, b: 4028, d: 11727\n",
      " -- global batches = 1611 --  w: 81373, b: 80604, d: 241303 (total: 403280)\n",
      "local batches: 80 --  w: 4219, b: 4073, d: 11879\n",
      "local batches: 81 --  w: 4274, b: 4130, d: 12015\n",
      "local batches: 82 --  w: 4326, b: 4192, d: 12149\n",
      "local batches: 83 --  w: 4380, b: 4253, d: 12282\n",
      "local batches: 84 --  w: 4441, b: 4308, d: 12414\n",
      " -- global batches = 1635 --  w: 82629, b: 81998, d: 244605 (total: 409232)\n",
      "local batches: 85 --  w: 4500, b: 4361, d: 12550\n",
      "local batches: 86 --  w: 4555, b: 4418, d: 12686\n",
      "local batches: 87 --  w: 4609, b: 4472, d: 12826\n",
      "local batches: 88 --  w: 4672, b: 4528, d: 12955\n",
      "local batches: 89 --  w: 4719, b: 4589, d: 13095\n",
      " -- global batches = 1660 --  w: 83907, b: 83476, d: 248049 (total: 415432)\n",
      "local batches: 90 --  w: 4764, b: 4644, d: 13243\n",
      "local batches: 91 --  w: 4822, b: 4700, d: 13377\n",
      "local batches: 92 --  w: 4865, b: 4757, d: 13525\n",
      "local batches: 93 --  w: 4912, b: 4819, d: 13664\n",
      "local batches: 94 --  w: 4962, b: 4875, d: 13806\n",
      " -- global batches = 1685 --  w: 85139, b: 84855, d: 251638 (total: 421632)\n",
      "local batches: 95 --  w: 5018, b: 4927, d: 13946\n",
      "local batches: 96 --  w: 5078, b: 4979, d: 14081\n",
      "local batches: 97 --  w: 5134, b: 5034, d: 14217\n",
      "local batches: 98 --  w: 5182, b: 5088, d: 14362\n",
      "local batches: 99 --  w: 5224, b: 5152, d: 14503\n",
      " -- global batches = 1710 --  w: 86455, b: 86237, d: 255123 (total: 427815)\n",
      "local batches: 100 --  w: 5277, b: 5215, d: 14634\n",
      "local batches: 101 --  w: 5321, b: 5286, d: 14766\n",
      "local batches: 102 --  w: 5375, b: 5340, d: 14905\n",
      "local batches: 103 --  w: 5426, b: 5396, d: 15045\n",
      "local batches: 104 --  w: 5465, b: 5450, d: 15199\n",
      " -- global batches = 1735 --  w: 87738, b: 87635, d: 258617 (total: 433990)\n",
      "local batches: 105 --  w: 5524, b: 5505, d: 15332\n",
      "local batches: 106 --  w: 5590, b: 5557, d: 15461\n",
      "local batches: 107 --  w: 5641, b: 5607, d: 15607\n",
      "local batches: 108 --  w: 5693, b: 5670, d: 15739\n",
      "local batches: 109 --  w: 5754, b: 5719, d: 15876\n",
      " -- global batches = 1759 --  w: 89063, b: 88928, d: 261927 (total: 439918)\n",
      "local batches: 110 --  w: 5819, b: 5778, d: 15999\n",
      "local batches: 111 --  w: 5864, b: 5842, d: 16137\n",
      "local batches: 112 --  w: 5915, b: 5897, d: 16277\n",
      "local batches: 113 --  w: 5957, b: 5953, d: 16425\n",
      "local batches: 114 --  w: 5992, b: 6015, d: 16574\n",
      " -- global batches = 1784 --  w: 90297, b: 90358, d: 265430 (total: 446085)\n",
      "local batches: 115 --  w: 6049, b: 6079, d: 16699\n",
      "local batches: 116 --  w: 6096, b: 6141, d: 16836\n",
      "local batches: 117 --  w: 6147, b: 6207, d: 16965\n",
      "local batches: 118 --  w: 6200, b: 6264, d: 17101\n",
      "local batches: 119 --  w: 6247, b: 6316, d: 17248\n",
      " -- global batches = 1809 --  w: 91606, b: 91786, d: 268843 (total: 452235)\n",
      "local batches: 120 --  w: 6299, b: 6377, d: 17381\n",
      "local batches: 121 --  w: 6364, b: 6436, d: 17503\n",
      "local batches: 122 --  w: 6426, b: 6499, d: 17624\n",
      "local batches: 123 --  w: 6475, b: 6560, d: 17760\n",
      "local batches: 124 --  w: 6534, b: 6619, d: 17888\n",
      " -- global batches = 1832 --  w: 92882, b: 93103, d: 271908 (total: 457893)\n",
      "local batches: 125 --  w: 6588, b: 6686, d: 18013\n",
      "local batches: 126 --  w: 6638, b: 6737, d: 18158\n",
      "local batches: 127 --  w: 6693, b: 6795, d: 18291\n",
      "local batches: 128 --  w: 6744, b: 6865, d: 18415\n",
      "local batches: 129 --  w: 6798, b: 6917, d: 18554\n",
      " -- global batches = 1857 --  w: 94279, b: 94493, d: 275261 (total: 464033)\n",
      "local batches: 130 --  w: 6851, b: 6971, d: 18692\n",
      "local batches: 131 --  w: 6904, b: 7026, d: 18829\n",
      "local batches: 132 --  w: 6969, b: 7074, d: 18961\n",
      "local batches: 133 --  w: 7023, b: 7121, d: 19105\n",
      "local batches: 134 --  w: 7096, b: 7170, d: 19228\n",
      " -- global batches = 1881 --  w: 95645, b: 95733, d: 278535 (total: 469913)\n",
      "local batches: 135 --  w: 7151, b: 7226, d: 19362\n",
      "local batches: 136 --  w: 7187, b: 7285, d: 19512\n",
      "local batches: 137 --  w: 7244, b: 7341, d: 19644\n",
      "local batches: 138 --  w: 7305, b: 7384, d: 19785\n",
      "local batches: 139 --  w: 7353, b: 7446, d: 19920\n",
      " -- global batches = 1906 --  w: 96974, b: 97056, d: 282008 (total: 476038)\n",
      "local batches: 140 --  w: 7401, b: 7505, d: 20058\n",
      "local batches: 141 --  w: 7450, b: 7550, d: 20209\n",
      "local batches: 142 --  w: 7496, b: 7595, d: 20363\n",
      "local batches: 143 --  w: 7546, b: 7656, d: 20497\n",
      "local batches: 144 --  w: 7615, b: 7705, d: 20624\n",
      " -- global batches = 1931 --  w: 98278, b: 98443, d: 285442 (total: 482163)\n",
      "local batches: 145 --  w: 7669, b: 7761, d: 20759\n",
      "local batches: 146 --  w: 7723, b: 7814, d: 20897\n",
      "local batches: 147 --  w: 7780, b: 7874, d: 21024\n",
      "local batches: 148 --  w: 7842, b: 7934, d: 21146\n",
      "local batches: 149 --  w: 7910, b: 7984, d: 21272\n",
      " -- global batches = 1956 --  w: 99755, b: 99816, d: 288703 (total: 488274)\n",
      "local batches: 150 --  w: 7954, b: 8037, d: 21419\n",
      "local batches: 151 --  w: 8022, b: 8076, d: 21556\n",
      "local batches: 152 --  w: 8078, b: 8134, d: 21686\n",
      "local batches: 153 --  w: 8140, b: 8194, d: 21808\n",
      "local batches: 154 --  w: 8190, b: 8244, d: 21952\n",
      " -- global batches = 1982 --  w: 101262, b: 101255, d: 292101 (total: 494618)\n",
      "local batches: 155 --  w: 8249, b: 8298, d: 22083\n",
      "local batches: 156 --  w: 8315, b: 8355, d: 22204\n",
      "local batches: 157 --  w: 8377, b: 8406, d: 22335\n",
      "local batches: 158 --  w: 8436, b: 8462, d: 22463\n",
      "local batches: 159 --  w: 8502, b: 8513, d: 22589\n",
      " -- global batches = 2007 --  w: 102754, b: 102663, d: 295291 (total: 500708)\n",
      "local batches: 160 --  w: 8555, b: 8567, d: 22725\n",
      "local batches: 161 --  w: 8616, b: 8629, d: 22845\n",
      "local batches: 162 --  w: 8670, b: 8686, d: 22977\n",
      "local batches: 163 --  w: 8735, b: 8734, d: 23107\n",
      "local batches: 164 --  w: 8784, b: 8783, d: 23252\n",
      " -- global batches = 2032 --  w: 104162, b: 104020, d: 298601 (total: 506783)\n",
      "local batches: 165 --  w: 8844, b: 8838, d: 23380\n",
      "local batches: 166 --  w: 8907, b: 8887, d: 23511\n",
      "local batches: 167 --  w: 8969, b: 8943, d: 23636\n",
      "local batches: 168 --  w: 9017, b: 8995, d: 23779\n",
      "local batches: 169 --  w: 9071, b: 9055, d: 23908\n",
      " -- global batches = 2056 --  w: 105489, b: 105350, d: 301776 (total: 512615)\n",
      "local batches: 170 --  w: 9118, b: 9101, d: 24058\n",
      "local batches: 171 --  w: 9168, b: 9149, d: 24203\n",
      "local batches: 172 --  w: 9223, b: 9215, d: 24325\n",
      "local batches: 173 --  w: 9286, b: 9268, d: 24452\n",
      "local batches: 174 --  w: 9349, b: 9320, d: 24579\n",
      " -- global batches = 2081 --  w: 106907, b: 106679, d: 305101 (total: 518687)\n",
      "local batches: 175 --  w: 9410, b: 9369, d: 24711\n",
      "local batches: 176 --  w: 9455, b: 9433, d: 24844\n",
      "local batches: 177 --  w: 9501, b: 9484, d: 24989\n",
      "local batches: 178 --  w: 9555, b: 9540, d: 25121\n",
      "local batches: 179 --  w: 9609, b: 9589, d: 25260\n",
      " -- global batches = 2106 --  w: 108257, b: 108011, d: 308469 (total: 524737)\n",
      "local batches: 180 --  w: 9667, b: 9647, d: 25386\n",
      "local batches: 181 --  w: 9729, b: 9700, d: 25513\n",
      "local batches: 182 --  w: 9787, b: 9745, d: 25652\n",
      "local batches: 183 --  w: 9845, b: 9798, d: 25783\n",
      "local batches: 184 --  w: 9889, b: 9861, d: 25918\n",
      " -- global batches = 2131 --  w: 109686, b: 109364, d: 311737 (total: 530787)\n",
      "local batches: 185 --  w: 9944, b: 9922, d: 26044\n",
      "local batches: 186 --  w: 9997, b: 9976, d: 26179\n",
      "local batches: 187 --  w: 10044, b: 10033, d: 26317\n",
      "local batches: 188 --  w: 10113, b: 10083, d: 26440\n",
      "local batches: 189 --  w: 10174, b: 10141, d: 26563\n",
      " -- global batches = 2155 --  w: 110986, b: 110637, d: 314972 (total: 536595)\n",
      "local batches: 190 --  w: 10232, b: 10194, d: 26694\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/Philip/Desktop/GitHub/ChessEngine/Train Value Function/Monte Carlo/ValueTrain_MC_v8_ParBatched_3.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v8_ParBatched_3.ipynb#W5sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m meta_moves[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v8_ParBatched_3.ipynb#W5sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     game\u001b[39m.\u001b[39mPlayMove(move)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v8_ParBatched_3.ipynb#W5sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     board_batch\u001b[39m.\u001b[39mappend(board_to_tensor(game\u001b[39m.\u001b[39;49mpieces))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v8_ParBatched_3.ipynb#W5sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     game\u001b[39m.\u001b[39mFlipBoard()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v8_ParBatched_3.ipynb#W5sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39mif\u001b[39;00m game\u001b[39m.\u001b[39mis_over():\n",
      "File \u001b[0;32m~/Desktop/GitHub/ChessEngine/Resources/Game.py:2127\u001b[0m, in \u001b[0;36mboard_to_tensor\u001b[0;34m(board)\u001b[0m\n\u001b[1;32m   2125\u001b[0m     tensor[i, :] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy((board \u001b[39m==\u001b[39m i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m   2126\u001b[0m     \u001b[39m# black pieces channel\u001b[39;00m\n\u001b[0;32m-> 2127\u001b[0m     tensor[i\u001b[39m+\u001b[39m\u001b[39m6\u001b[39m, :] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy((board \u001b[39m==\u001b[39;49m i\u001b[39m+\u001b[39;49m\u001b[39m11\u001b[39;49m)\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39muint8\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m   2129\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# keep generating new batches of data until stopped\n",
    "while True:\n",
    "\n",
    "    batch_white_wins = 0\n",
    "    batch_black_wins = 0\n",
    "    batch_draws = 0\n",
    "\n",
    "    # load newest model\n",
    "    model = Model_v8()\n",
    "    model_saves = os.listdir('../Monte Carlo/Model Saves MC v8_3')\n",
    "    if len(model_saves) > 0:\n",
    "        newest_model = max(int(i[6:-8]) for i in model_saves)\n",
    "        model.load_state_dict(torch.load('../Monte Carlo/Model Saves MC v8_3/model_{}_batches'.format(newest_model)))\n",
    "    else:\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    # print('loaded model ', newest_model)\n",
    "\n",
    "    with open('/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v8_3/stats', 'rb') as f:\n",
    "        stats = torch.load(f)\n",
    "    stats = stats.int()\n",
    "    global_white_wins   = stats[1]\n",
    "    global_black_wins   = stats[2]\n",
    "    global_draws        = stats[3]\n",
    "\n",
    "    percentage_decisive = (global_white_wins + global_black_wins) / (global_white_wins + global_black_wins + global_draws)\n",
    "    batch_size = int(batch_target // percentage_decisive.item()) # so that on average we have [batch_target] decisive games\n",
    "\n",
    "    meta_games = []\n",
    "    meta_boards_white = []; meta_boards_black = []\n",
    "    i = 0\n",
    "\n",
    "    meta_active = []\n",
    "\n",
    "    for game_ind in range(batch_size):\n",
    "        meta_games.append( Game() )\n",
    "        meta_boards_white.append([]); meta_boards_black.append([])\n",
    "        meta_active.append(True)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # print('start with {} games'.format(batch_size))\n",
    "\n",
    "    while True in meta_active:\n",
    "\n",
    "        # if i % 20 == 0:\n",
    "        #     print('i = {}, with {} active games '.format(i, meta_active.count(True)))\n",
    "\n",
    "        i += 1\n",
    "        full_board_batch = []\n",
    "        meta_board_batch_sizes = [] # save batch sizes to split model output afterwards\n",
    "        meta_moves = []\n",
    "\n",
    "        # go through games, collect positions for value evaluations\n",
    "        for g, game in enumerate(meta_games):\n",
    "\n",
    "            if not meta_active[g]:\n",
    "                meta_board_batch_sizes.append(0)\n",
    "                meta_moves.append([])\n",
    "                continue\n",
    "\n",
    "            meta_moves.append(game.PossibleMoves())\n",
    "            game_ini = game.copy()\n",
    "            board_batch = [board_to_tensor(game.pieces)]\n",
    "            mate = False\n",
    "\n",
    "            for move in meta_moves[-1]:\n",
    "                game.PlayMove(move)\n",
    "                board_batch.append(board_to_tensor(game.pieces))\n",
    "                game.FlipBoard()\n",
    "                if game.is_over():\n",
    "                    mate = True\n",
    "                    break\n",
    "                game = game_ini.copy()\n",
    "\n",
    "            meta_games[g] = game.copy()\n",
    "\n",
    "            if mate:\n",
    "                meta_active[g] = False\n",
    "                meta_board_batch_sizes.append(0)\n",
    "                game.FlipBoard()\n",
    "                if i % 2 == 1:\n",
    "                    meta_boards_white[g].append(board_to_bool_tensor(game.pieces))\n",
    "                if i % 2 == 0:\n",
    "                    meta_boards_black[g].append(board_to_bool_tensor(game.pieces))\n",
    "\n",
    "            if not mate:\n",
    "                full_board_batch = full_board_batch + board_batch\n",
    "                meta_board_batch_sizes.append(len(board_batch))\n",
    "\n",
    "        if len(full_board_batch) == 0:\n",
    "            break\n",
    "\n",
    "        # get values of all positions\n",
    "        full_board_batch = torch.stack(full_board_batch)\n",
    "        out = model(full_board_batch).detach()\n",
    "        meta_values = torch.split(out, meta_board_batch_sizes)\n",
    "\n",
    "        # make moves for all games\n",
    "        for g, game in enumerate(meta_games):\n",
    "\n",
    "            if not meta_active[g]:\n",
    "                continue\n",
    "            values = meta_values[g]\n",
    "            if i < 7:\n",
    "                scale = value_diff_scale_early\n",
    "            else:\n",
    "                scale = value_diff_scale\n",
    "            values_diff = [scale*(values[i] - values[0]) for i in range(1, len(values))]\n",
    "            move_prob = torch.softmax(torch.Tensor(values_diff), dim=0).numpy()\n",
    "            chosen_i = np.random.choice(range(len(meta_moves[g])), p=move_prob)\n",
    "            chosen_move = meta_moves[g][chosen_i]\n",
    "            game.PlayMove(chosen_move)\n",
    "            if i % 2 == 1:\n",
    "                meta_boards_white[g].append(board_to_bool_tensor(game.pieces))\n",
    "            if i % 2 == 0:\n",
    "                meta_boards_black[g].append(board_to_bool_tensor(game.pieces))\n",
    "            game.FlipBoard()\n",
    "\n",
    "    # print('games done, start evaluating')\n",
    "\n",
    "    meta_inputs = []\n",
    "    meta_labels = []\n",
    "\n",
    "    for g, game in enumerate(meta_games):\n",
    "        \n",
    "        winner = game.get_winner()\n",
    "        if winner == 'draw':\n",
    "            draws += 1; batch_draws += 1\n",
    "            continue\n",
    "            reward_white = 0;   reward_black = 0\n",
    "\n",
    "        elif winner == 'white':\n",
    "            white_wins += 1; batch_white_wins += 1\n",
    "            reward_white = 1;   reward_black = -1\n",
    "\n",
    "        elif winner == 'black':\n",
    "            black_wins += 1; batch_black_wins += 1\n",
    "            reward_white = -1;  reward_black = 1\n",
    "\n",
    "        labels_white = [reward_white * gamma**(len(meta_boards_white[g]) - 1 - i) for i in range(len(meta_boards_white[g]))]\n",
    "        labels_black = [reward_black * gamma**(len(meta_boards_black[g]) - 1 - i) for i in range(len(meta_boards_black[g]))]\n",
    "\n",
    "        meta_inputs = meta_inputs + meta_boards_white[g] + meta_boards_black[g]\n",
    "        meta_labels = meta_labels + labels_white + labels_black\n",
    "\n",
    "    if batch_white_wins + batch_black_wins == 0:\n",
    "        print('no decisive games in the whole batch -> skip to next batch (batch size too small?)')\n",
    "        continue\n",
    "\n",
    "    inputs_tens = torch.stack(meta_inputs)\n",
    "    labels_tens = torch.Tensor(meta_labels)\n",
    "\n",
    "    # print('evaluation done, save batch')\n",
    "\n",
    "    with open('/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v8_3/stats', 'rb') as f:\n",
    "        stats = torch.load(f)\n",
    "    stats = stats.int()\n",
    "    stats[0] += 1;                  stats[1] += batch_white_wins\n",
    "    stats[2] += batch_black_wins;   stats[3] += batch_draws\n",
    "    torch.save(stats, '/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v8_3/stats')\n",
    "\n",
    "    # print('batch index = ', stats[0])\n",
    "\n",
    "    new_batch_index = stats[0]\n",
    "    torch.save(inputs_tens, '/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v8_3/inputs_{}'.format(new_batch_index))\n",
    "    torch.save(labels_tens, '/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v8_3/labels_{}'.format(new_batch_index))\n",
    "\n",
    "    print('local batches: {} --  w: {}, b: {}, d: {}'.format(batch_count, white_wins, black_wins, draws))\n",
    "\n",
    "    batch_count += 1\n",
    "\n",
    "    if batch_count % 5 == 0:\n",
    "        print(' -- global batches = {} --  w: {}, b: {}, d: {} (total: {})'.format(\n",
    "            new_batch_index, stats[1], stats[2], stats[3], stats[1] + stats[2] + stats[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
