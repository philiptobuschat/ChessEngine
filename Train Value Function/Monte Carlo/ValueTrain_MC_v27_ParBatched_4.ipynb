{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Resources.Model import Model_v27\n",
    "from Resources.Game import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.98\n",
    "\n",
    "def scale(i):\n",
    "    return i\n",
    "\n",
    "# games saved in batches to reduce i/o stream\n",
    "# each batch is a input file and a label file containing [batch_size] individual games\n",
    "batch_target = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### local counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_wins = 0\n",
    "black_wins = 0\n",
    "draws = 0\n",
    "\n",
    "batch_count = 0         # number of batches locally done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local batches: 4 --  w: 572, b: 537, d: 511\n",
      " -- global batches = 682 --  w: 64541, b: 64594, d: 183146 (total: 312281)\n",
      "local batches: 5 --  w: 666, b: 637, d: 800\n",
      "local batches: 6 --  w: 761, b: 733, d: 1092\n",
      "local batches: 7 --  w: 854, b: 838, d: 1377\n",
      "local batches: 8 --  w: 959, b: 951, d: 1643\n",
      "local batches: 9 --  w: 1045, b: 1054, d: 1937\n",
      " -- global batches = 702 --  w: 66490, b: 66569, d: 188885 (total: 321944)\n",
      "local batches: 10 --  w: 1133, b: 1131, d: 2255\n",
      "local batches: 11 --  w: 1248, b: 1222, d: 2533\n",
      "local batches: 12 --  w: 1327, b: 1312, d: 2848\n",
      "local batches: 13 --  w: 1411, b: 1405, d: 3155\n",
      "local batches: 14 --  w: 1520, b: 1505, d: 3430\n",
      " -- global batches = 722 --  w: 68373, b: 68462, d: 194784 (total: 331619)\n",
      "local batches: 15 --  w: 1609, b: 1603, d: 3727\n",
      "local batches: 16 --  w: 1714, b: 1698, d: 4011\n",
      "local batches: 17 --  w: 1806, b: 1795, d: 4306\n",
      "local batches: 18 --  w: 1890, b: 1906, d: 4595\n",
      "local batches: 19 --  w: 1985, b: 1999, d: 4892\n",
      " -- global batches = 742 --  w: 70286, b: 70412, d: 200604 (total: 341302)\n",
      "local batches: 20 --  w: 2077, b: 2081, d: 5203\n",
      "local batches: 21 --  w: 2173, b: 2196, d: 5477\n",
      "local batches: 22 --  w: 2258, b: 2283, d: 5790\n",
      "local batches: 23 --  w: 2341, b: 2369, d: 6106\n",
      "local batches: 24 --  w: 2437, b: 2479, d: 6385\n",
      " -- global batches = 762 --  w: 72172, b: 72377, d: 206453 (total: 351002)\n",
      "local batches: 25 --  w: 2520, b: 2568, d: 6698\n",
      "local batches: 26 --  w: 2620, b: 2663, d: 6988\n",
      "local batches: 27 --  w: 2704, b: 2749, d: 7304\n",
      "local batches: 28 --  w: 2801, b: 2828, d: 7614\n",
      "local batches: 29 --  w: 2898, b: 2917, d: 7914\n",
      " -- global batches = 783 --  w: 74176, b: 74277, d: 212745 (total: 361198)\n",
      "local batches: 30 --  w: 2995, b: 3006, d: 8214\n",
      "local batches: 31 --  w: 3084, b: 3087, d: 8530\n",
      "local batches: 32 --  w: 3185, b: 3163, d: 8839\n",
      "local batches: 33 --  w: 3263, b: 3249, d: 9162\n",
      "local batches: 34 --  w: 3345, b: 3335, d: 9481\n",
      " -- global batches = 803 --  w: 76062, b: 76072, d: 218792 (total: 370926)\n",
      "local batches: 35 --  w: 3451, b: 3403, d: 9794\n",
      "local batches: 36 --  w: 3540, b: 3501, d: 10094\n",
      "local batches: 37 --  w: 3641, b: 3596, d: 10385\n",
      "local batches: 38 --  w: 3728, b: 3696, d: 10685\n",
      "local batches: 39 --  w: 3817, b: 3772, d: 11007\n",
      " -- global batches = 823 --  w: 78050, b: 77953, d: 224663 (total: 380666)\n",
      "local batches: 40 --  w: 3909, b: 3840, d: 11335\n",
      "local batches: 41 --  w: 4003, b: 3928, d: 11641\n",
      "local batches: 42 --  w: 4110, b: 4010, d: 11940\n",
      "local batches: 43 --  w: 4199, b: 4094, d: 12256\n",
      "local batches: 44 --  w: 4297, b: 4209, d: 12532\n",
      " -- global batches = 843 --  w: 79786, b: 79682, d: 230962 (total: 390430)\n",
      "local batches: 45 --  w: 4389, b: 4297, d: 12841\n",
      "local batches: 46 --  w: 4460, b: 4384, d: 13173\n",
      "local batches: 47 --  w: 4535, b: 4472, d: 13500\n",
      "local batches: 48 --  w: 4627, b: 4550, d: 13820\n",
      "local batches: 49 --  w: 4727, b: 4629, d: 14131\n",
      " -- global batches = 863 --  w: 81574, b: 81391, d: 237259 (total: 400224)\n",
      "local batches: 50 --  w: 4835, b: 4722, d: 14421\n",
      "local batches: 51 --  w: 4929, b: 4801, d: 14739\n",
      "local batches: 52 --  w: 5020, b: 4894, d: 15046\n",
      "local batches: 53 --  w: 5103, b: 4961, d: 15387\n",
      "local batches: 54 --  w: 5198, b: 5061, d: 15683\n",
      " -- global batches = 883 --  w: 83388, b: 83194, d: 243461 (total: 410043)\n",
      "local batches: 55 --  w: 5299, b: 5150, d: 15985\n",
      "local batches: 56 --  w: 5396, b: 5232, d: 16298\n",
      "local batches: 57 --  w: 5480, b: 5323, d: 16615\n",
      "local batches: 58 --  w: 5575, b: 5422, d: 16913\n",
      "local batches: 59 --  w: 5658, b: 5516, d: 17228\n",
      " -- global batches = 903 --  w: 85255, b: 85061, d: 249567 (total: 419883)\n",
      "local batches: 60 --  w: 5747, b: 5597, d: 17551\n",
      "local batches: 61 --  w: 5829, b: 5691, d: 17868\n",
      "local batches: 62 --  w: 5927, b: 5780, d: 18174\n",
      "local batches: 63 --  w: 6013, b: 5869, d: 18492\n",
      "local batches: 64 --  w: 6116, b: 5970, d: 18782\n",
      " -- global batches = 923 --  w: 87056, b: 86817, d: 255871 (total: 429744)\n",
      "local batches: 65 --  w: 6206, b: 6068, d: 19088\n",
      "local batches: 66 --  w: 6301, b: 6150, d: 19405\n",
      "local batches: 67 --  w: 6391, b: 6248, d: 19711\n",
      "local batches: 68 --  w: 6483, b: 6353, d: 20008\n",
      "local batches: 69 --  w: 6576, b: 6441, d: 20321\n",
      " -- global batches = 943 --  w: 88978, b: 88735, d: 261911 (total: 439624)\n",
      "local batches: 70 --  w: 6650, b: 6527, d: 20655\n",
      "local batches: 71 --  w: 6746, b: 6607, d: 20973\n",
      "local batches: 72 --  w: 6836, b: 6707, d: 21278\n",
      "local batches: 73 --  w: 6939, b: 6812, d: 21565\n",
      "local batches: 74 --  w: 7039, b: 6910, d: 21862\n",
      " -- global batches = 963 --  w: 90863, b: 90575, d: 268078 (total: 449516)\n",
      "local batches: 75 --  w: 7122, b: 7005, d: 22179\n",
      "local batches: 76 --  w: 7220, b: 7089, d: 22492\n",
      "local batches: 77 --  w: 7308, b: 7177, d: 22811\n",
      "local batches: 78 --  w: 7390, b: 7262, d: 23140\n",
      "local batches: 79 --  w: 7495, b: 7365, d: 23428\n",
      " -- global batches = 983 --  w: 92727, b: 92363, d: 274332 (total: 459422)\n",
      "local batches: 80 --  w: 7577, b: 7484, d: 23723\n",
      "local batches: 81 --  w: 7676, b: 7570, d: 24034\n",
      "local batches: 82 --  w: 7766, b: 7686, d: 24324\n",
      "local batches: 83 --  w: 7860, b: 7772, d: 24641\n",
      "local batches: 84 --  w: 7949, b: 7872, d: 24949\n",
      " -- global batches = 1003 --  w: 94542, b: 94261, d: 280544 (total: 469347)\n",
      "local batches: 85 --  w: 8045, b: 7963, d: 25259\n",
      "local batches: 86 --  w: 8138, b: 8053, d: 25573\n",
      "local batches: 87 --  w: 8237, b: 8145, d: 25879\n",
      "local batches: 88 --  w: 8343, b: 8234, d: 26181\n",
      "local batches: 89 --  w: 8434, b: 8337, d: 26484\n",
      " -- global batches = 1023 --  w: 96415, b: 96130, d: 286742 (total: 479287)\n",
      "local batches: 90 --  w: 8527, b: 8424, d: 26801\n",
      "local batches: 91 --  w: 8632, b: 8515, d: 27103\n",
      "local batches: 92 --  w: 8734, b: 8606, d: 27408\n",
      "local batches: 93 --  w: 8826, b: 8691, d: 27729\n",
      "local batches: 94 --  w: 8926, b: 8815, d: 28003\n",
      " -- global batches = 1043 --  w: 98280, b: 97968, d: 292992 (total: 489240)\n",
      "local batches: 95 --  w: 9033, b: 8911, d: 28298\n",
      "local batches: 96 --  w: 9130, b: 8991, d: 28619\n",
      "local batches: 97 --  w: 9228, b: 9076, d: 28934\n",
      "local batches: 98 --  w: 9308, b: 9161, d: 29268\n",
      "local batches: 99 --  w: 9413, b: 9243, d: 29580\n",
      " -- global batches = 1063 --  w: 100201, b: 99704, d: 299300 (total: 499205)\n",
      "local batches: 100 --  w: 9511, b: 9338, d: 29886\n",
      "local batches: 101 --  w: 9611, b: 9422, d: 30201\n",
      "local batches: 102 --  w: 9700, b: 9512, d: 30521\n",
      "local batches: 103 --  w: 9796, b: 9598, d: 30838\n",
      "local batches: 104 --  w: 9895, b: 9702, d: 31134\n",
      " -- global batches = 1083 --  w: 102105, b: 101586, d: 305494 (total: 509185)\n",
      "local batches: 105 --  w: 9990, b: 9790, d: 31450\n",
      "local batches: 106 --  w: 10100, b: 9902, d: 31728\n",
      "local batches: 107 --  w: 10174, b: 9989, d: 32066\n",
      "local batches: 108 --  w: 10286, b: 10094, d: 32349\n",
      "local batches: 109 --  w: 10380, b: 10175, d: 32674\n",
      " -- global batches = 1103 --  w: 104051, b: 103511, d: 311615 (total: 519177)\n",
      "local batches: 110 --  w: 10487, b: 10278, d: 32964\n",
      "local batches: 111 --  w: 10591, b: 10365, d: 33273\n",
      "local batches: 112 --  w: 10701, b: 10448, d: 33580\n",
      "local batches: 113 --  w: 10797, b: 10531, d: 33901\n",
      "local batches: 114 --  w: 10889, b: 10618, d: 34222\n",
      " -- global batches = 1123 --  w: 106028, b: 105401, d: 317748 (total: 529177)\n",
      "local batches: 115 --  w: 11010, b: 10705, d: 34514\n",
      "local batches: 116 --  w: 11100, b: 10795, d: 34834\n",
      "local batches: 117 --  w: 11192, b: 10893, d: 35144\n",
      "local batches: 118 --  w: 11301, b: 11020, d: 35408\n",
      "local batches: 119 --  w: 11412, b: 11108, d: 35709\n",
      " -- global batches = 1143 --  w: 107996, b: 107325, d: 323856 (total: 539177)\n",
      "local batches: 120 --  w: 11516, b: 11184, d: 36029\n",
      "local batches: 121 --  w: 11607, b: 11267, d: 36356\n",
      "local batches: 122 --  w: 11689, b: 11362, d: 36680\n",
      "local batches: 123 --  w: 11794, b: 11458, d: 36980\n",
      "local batches: 124 --  w: 11893, b: 11537, d: 37303\n",
      " -- global batches = 1163 --  w: 109917, b: 109059, d: 330214 (total: 549190)\n",
      "local batches: 125 --  w: 11989, b: 11624, d: 37621\n",
      "local batches: 126 --  w: 12085, b: 11700, d: 37950\n",
      "local batches: 127 --  w: 12180, b: 11776, d: 38280\n",
      "local batches: 128 --  w: 12289, b: 11866, d: 38582\n",
      "local batches: 129 --  w: 12388, b: 11954, d: 38897\n",
      " -- global batches = 1184 --  w: 111908, b: 111021, d: 336784 (total: 559713)\n",
      "local batches: 130 --  w: 12488, b: 12063, d: 39190\n",
      "local batches: 131 --  w: 12572, b: 12157, d: 39514\n",
      "local batches: 132 --  w: 12662, b: 12280, d: 39803\n",
      "local batches: 133 --  w: 12776, b: 12371, d: 40100\n",
      "local batches: 134 --  w: 12865, b: 12461, d: 40423\n",
      " -- global batches = 1204 --  w: 113789, b: 113051, d: 342913 (total: 569753)\n",
      "local batches: 135 --  w: 12967, b: 12553, d: 40731\n",
      "local batches: 136 --  w: 13059, b: 12644, d: 41050\n",
      "local batches: 137 --  w: 13162, b: 12727, d: 41366\n",
      "local batches: 138 --  w: 13267, b: 12827, d: 41663\n",
      "local batches: 139 --  w: 13353, b: 12919, d: 41987\n",
      " -- global batches = 1224 --  w: 115652, b: 114873, d: 349268 (total: 579793)\n",
      "local batches: 140 --  w: 13429, b: 13002, d: 42331\n",
      "local batches: 141 --  w: 13531, b: 13104, d: 42630\n",
      "local batches: 142 --  w: 13630, b: 13200, d: 42938\n",
      "local batches: 143 --  w: 13724, b: 13305, d: 43242\n",
      "local batches: 144 --  w: 13812, b: 13396, d: 43566\n",
      " -- global batches = 1244 --  w: 117548, b: 116735, d: 355567 (total: 589850)\n",
      "local batches: 145 --  w: 13904, b: 13480, d: 43893\n",
      "local batches: 146 --  w: 14000, b: 13589, d: 44191\n",
      "local batches: 147 --  w: 14112, b: 13700, d: 44471\n",
      "local batches: 148 --  w: 14211, b: 13797, d: 44778\n",
      "local batches: 149 --  w: 14312, b: 13897, d: 45080\n",
      " -- global batches = 1263 --  w: 119356, b: 118545, d: 361506 (total: 599407)\n",
      "local batches: 150 --  w: 14395, b: 14004, d: 45393\n",
      "local batches: 151 --  w: 14492, b: 14095, d: 45708\n",
      "local batches: 152 --  w: 14577, b: 14179, d: 46043\n",
      "local batches: 153 --  w: 14693, b: 14274, d: 46336\n",
      "local batches: 154 --  w: 14785, b: 14359, d: 46663\n",
      " -- global batches = 1283 --  w: 121337, b: 120440, d: 367699 (total: 609476)\n",
      "local batches: 155 --  w: 14882, b: 14429, d: 47000\n",
      "local batches: 156 --  w: 14988, b: 14521, d: 47306\n",
      "local batches: 157 --  w: 15083, b: 14637, d: 47599\n",
      "local batches: 158 --  w: 15170, b: 14737, d: 47916\n",
      "local batches: 159 --  w: 15265, b: 14834, d: 48228\n",
      " -- global batches = 1304 --  w: 123335, b: 122425, d: 374300 (total: 620060)\n",
      "local batches: 160 --  w: 15363, b: 14937, d: 48531\n",
      "local batches: 161 --  w: 15453, b: 15038, d: 48844\n",
      "local batches: 162 --  w: 15569, b: 15134, d: 49136\n",
      "local batches: 163 --  w: 15676, b: 15231, d: 49436\n",
      "local batches: 164 --  w: 15771, b: 15329, d: 49747\n",
      " -- global batches = 1324 --  w: 125379, b: 124401, d: 380360 (total: 630140)\n",
      "local batches: 165 --  w: 15874, b: 15434, d: 50043\n",
      "local batches: 166 --  w: 15973, b: 15527, d: 50355\n",
      "local batches: 167 --  w: 16081, b: 15624, d: 50654\n",
      "local batches: 168 --  w: 16177, b: 15706, d: 50980\n",
      "local batches: 169 --  w: 16269, b: 15801, d: 51297\n",
      " -- global batches = 1344 --  w: 127425, b: 126288, d: 386507 (total: 640220)\n",
      "local batches: 170 --  w: 16364, b: 15901, d: 51606\n",
      "local batches: 171 --  w: 16471, b: 16011, d: 51893\n",
      "local batches: 172 --  w: 16584, b: 16104, d: 52191\n",
      "local batches: 173 --  w: 16694, b: 16220, d: 52469\n",
      "local batches: 174 --  w: 16769, b: 16320, d: 52798\n",
      " -- global batches = 1364 --  w: 129466, b: 128266, d: 392568 (total: 650300)\n",
      "local batches: 175 --  w: 16863, b: 16408, d: 53120\n",
      "local batches: 176 --  w: 16943, b: 16516, d: 53436\n",
      "local batches: 177 --  w: 17028, b: 16626, d: 53745\n",
      "local batches: 178 --  w: 17132, b: 16725, d: 54046\n",
      "local batches: 179 --  w: 17239, b: 16818, d: 54350\n",
      " -- global batches = 1383 --  w: 131348, b: 130184, d: 398344 (total: 659876)\n",
      "local batches: 180 --  w: 17336, b: 16935, d: 54640\n",
      "local batches: 181 --  w: 17419, b: 17026, d: 54970\n",
      "local batches: 182 --  w: 17508, b: 17142, d: 55269\n",
      "local batches: 183 --  w: 17622, b: 17245, d: 55556\n",
      "local batches: 184 --  w: 17693, b: 17348, d: 55886\n",
      " -- global batches = 1403 --  w: 133284, b: 132216, d: 404456 (total: 669956)\n",
      "local batches: 185 --  w: 17780, b: 17455, d: 56196\n",
      "local batches: 186 --  w: 17889, b: 17547, d: 56499\n",
      "local batches: 187 --  w: 18009, b: 17643, d: 56787\n",
      "local batches: 188 --  w: 18111, b: 17735, d: 57097\n",
      "local batches: 189 --  w: 18218, b: 17824, d: 57405\n",
      " -- global batches = 1423 --  w: 135272, b: 134193, d: 410571 (total: 680036)\n",
      "local batches: 190 --  w: 18325, b: 17915, d: 57711\n",
      "local batches: 191 --  w: 18413, b: 18005, d: 58037\n",
      "local batches: 192 --  w: 18514, b: 18113, d: 58332\n",
      "local batches: 193 --  w: 18626, b: 18202, d: 58635\n",
      "local batches: 194 --  w: 18719, b: 18290, d: 58958\n",
      " -- global batches = 1443 --  w: 137265, b: 136174, d: 416677 (total: 690116)\n",
      "local batches: 195 --  w: 18813, b: 18391, d: 59267\n",
      "local batches: 196 --  w: 18924, b: 18507, d: 59544\n",
      "local batches: 197 --  w: 19009, b: 18622, d: 59848\n",
      "local batches: 198 --  w: 19129, b: 18708, d: 60146\n",
      "local batches: 199 --  w: 19247, b: 18799, d: 60441\n",
      " -- global batches = 1463 --  w: 139247, b: 138236, d: 422713 (total: 700196)\n",
      "local batches: 200 --  w: 19341, b: 18901, d: 60749\n",
      "local batches: 201 --  w: 19453, b: 19002, d: 61040\n",
      "local batches: 202 --  w: 19545, b: 19102, d: 61352\n",
      "local batches: 203 --  w: 19653, b: 19209, d: 61641\n",
      "local batches: 204 --  w: 19747, b: 19301, d: 61959\n",
      " -- global batches = 1484 --  w: 141430, b: 140345, d: 429005 (total: 710780)\n",
      "local batches: 205 --  w: 19847, b: 19399, d: 62265\n",
      "local batches: 206 --  w: 19961, b: 19489, d: 62565\n",
      "local batches: 207 --  w: 20072, b: 19605, d: 62842\n",
      "local batches: 208 --  w: 20171, b: 19709, d: 63143\n",
      "local batches: 209 --  w: 20288, b: 19816, d: 63423\n",
      " -- global batches = 1504 --  w: 143566, b: 142469, d: 434825 (total: 720860)\n",
      "local batches: 210 --  w: 20407, b: 19934, d: 63690\n",
      "local batches: 211 --  w: 20513, b: 20037, d: 63984\n",
      "local batches: 212 --  w: 20611, b: 20124, d: 64302\n",
      "local batches: 213 --  w: 20705, b: 20237, d: 64598\n",
      "local batches: 214 --  w: 20807, b: 20347, d: 64889\n",
      " -- global batches = 1524 --  w: 145673, b: 144500, d: 440752 (total: 730925)\n",
      "local batches: 215 --  w: 20919, b: 20453, d: 65174\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/Philip/Desktop/GitHub/ChessEngine/Train Value Function/Monte Carlo/ValueTrain_MC_v27_ParBatched_4.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v27_ParBatched_4.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(model_saves) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v27_ParBatched_4.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     newest_model \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mint\u001b[39m(i[\u001b[39m6\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m8\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m model_saves)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v27_ParBatched_4.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m../Monte Carlo/Model Saves MC v27/model_\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m_batches\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(newest_model)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v27_ParBatched_4.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Monte%20Carlo/ValueTrain_MC_v27_ParBatched_4.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.05\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:797\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m     \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m     \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m     \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n\u001b[0;32m--> 797\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_reader(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    798\u001b[0m         \u001b[39mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m    799\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    800\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39m dispatching to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.jit.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (call \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.jit.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directly to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    801\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39m silence this warning)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:283\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name_or_buffer) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileReader(name_or_buffer))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "# keep generating new batches of data until stopped\n",
    "while True:\n",
    "\n",
    "    batch_white_wins = 0\n",
    "    batch_black_wins = 0\n",
    "    batch_draws = 0\n",
    "\n",
    "    # load newest model\n",
    "    model = Model_v27()\n",
    "    model_saves = os.listdir('../Monte Carlo/Model Saves MC v27')\n",
    "    if len(model_saves) > 0:\n",
    "        newest_model = max(int(i[6:-8]) for i in model_saves)\n",
    "        model.load_state_dict(torch.load('../Monte Carlo/Model Saves MC v27/model_{}_batches'.format(newest_model)))\n",
    "    else:\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    # print('loaded model ', newest_model)\n",
    "\n",
    "    with open('/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v27/stats', 'rb') as f:\n",
    "        stats = torch.load(f)\n",
    "    stats = stats.int()\n",
    "    global_white_wins   = stats[1]\n",
    "    global_black_wins   = stats[2]\n",
    "    global_draws        = stats[3]\n",
    "\n",
    "    if (global_white_wins + global_black_wins + global_draws) == 0:\n",
    "        percentage_decisive = 0.5\n",
    "    else:\n",
    "        percentage_decisive = ((global_white_wins + global_black_wins) / (global_white_wins + global_black_wins + global_draws)).item()\n",
    "    \n",
    "    batch_size = int(batch_target // percentage_decisive) # so that on average we have [batch_target] decisive games\n",
    "\n",
    "    meta_games = []\n",
    "    meta_boards_white = []; meta_boards_black = []\n",
    "    i = 0\n",
    "\n",
    "    meta_active = []\n",
    "\n",
    "    for game_ind in range(batch_size):\n",
    "        meta_games.append( Game() )\n",
    "        meta_boards_white.append([]); meta_boards_black.append([])\n",
    "        meta_active.append(True)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    while True in meta_active:\n",
    "\n",
    "        i += 1\n",
    "        full_board_batch = []\n",
    "        meta_board_batch_sizes = [] # save batch sizes to split model output afterwards\n",
    "        meta_moves = []\n",
    "\n",
    "        # go through games, collect positions for value evaluations\n",
    "        for g, game in enumerate(meta_games):\n",
    "\n",
    "            if not meta_active[g]:\n",
    "                meta_board_batch_sizes.append(0)\n",
    "                meta_moves.append([])\n",
    "                continue\n",
    "\n",
    "            meta_moves.append(game.PossibleMoves())\n",
    "            game_ini = game.copy()\n",
    "            board_batch = [board_to_tensor(game.pieces)]\n",
    "            mate = False\n",
    "\n",
    "            for move in meta_moves[-1]:\n",
    "                game.PlayMove(move)\n",
    "                board_batch.append(board_to_tensor(game.pieces))\n",
    "                game.FlipBoard()\n",
    "                if game.is_over():\n",
    "                    mate = True\n",
    "                    break\n",
    "                game = game_ini.copy()\n",
    "\n",
    "            meta_games[g] = game.copy()\n",
    "\n",
    "            if mate:\n",
    "                meta_active[g] = False\n",
    "                meta_board_batch_sizes.append(0)\n",
    "                game.FlipBoard()\n",
    "                if i % 2 == 1:\n",
    "                    meta_boards_white[g].append(board_to_bool_tensor(game.pieces))\n",
    "                if i % 2 == 0:\n",
    "                    meta_boards_black[g].append(board_to_bool_tensor(game.pieces))\n",
    "\n",
    "            if not mate:\n",
    "                full_board_batch = full_board_batch + board_batch\n",
    "                meta_board_batch_sizes.append(len(board_batch))\n",
    "\n",
    "        if len(full_board_batch) == 0:\n",
    "            break\n",
    "\n",
    "        # get values of all positions\n",
    "        full_board_batch = torch.stack(full_board_batch)\n",
    "        out = model(full_board_batch).detach()\n",
    "        meta_values = torch.split(out, meta_board_batch_sizes)\n",
    "\n",
    "        # make moves for all games\n",
    "        for g, game in enumerate(meta_games):\n",
    "\n",
    "            if not meta_active[g]:\n",
    "                continue\n",
    "            values = meta_values[g]\n",
    "            values_diff = [scale(i)*(values[j] - values[0]) for j in range(1, len(values))]\n",
    "            move_prob = torch.softmax(torch.Tensor(values_diff), dim=0).numpy()\n",
    "            chosen_i = np.random.choice(range(len(meta_moves[g])), p=move_prob)\n",
    "            chosen_move = meta_moves[g][chosen_i]\n",
    "            game.PlayMove(chosen_move)\n",
    "            if i % 2 == 1:\n",
    "                meta_boards_white[g].append(board_to_bool_tensor(game.pieces))\n",
    "            if i % 2 == 0:\n",
    "                meta_boards_black[g].append(board_to_bool_tensor(game.pieces))\n",
    "            game.FlipBoard()\n",
    "\n",
    "    meta_inputs = []\n",
    "    meta_labels = []\n",
    "\n",
    "    for g, game in enumerate(meta_games):\n",
    "        \n",
    "        winner = game.get_winner()\n",
    "        if winner == 'draw':\n",
    "            draws += 1; batch_draws += 1\n",
    "            continue\n",
    "            reward_white = 0;   reward_black = 0\n",
    "\n",
    "        elif winner == 'white':\n",
    "            white_wins += 1; batch_white_wins += 1\n",
    "            reward_white = 1;   reward_black = -1\n",
    "\n",
    "        elif winner == 'black':\n",
    "            black_wins += 1; batch_black_wins += 1\n",
    "            reward_white = -1;  reward_black = 1\n",
    "\n",
    "        labels_white = [reward_white * gamma**(len(meta_boards_white[g]) - 1 - i) for i in range(len(meta_boards_white[g]))]\n",
    "        labels_black = [reward_black * gamma**(len(meta_boards_black[g]) - 1 - i) for i in range(len(meta_boards_black[g]))]\n",
    "\n",
    "        meta_inputs = meta_inputs + meta_boards_white[g] + meta_boards_black[g]\n",
    "        meta_labels = meta_labels + labels_white + labels_black\n",
    "\n",
    "    if batch_white_wins + batch_black_wins == 0:\n",
    "        print('no decisive games in the whole batch -> skip to next batch (batch size too small?)')\n",
    "        continue\n",
    "\n",
    "    inputs_tens = torch.stack(meta_inputs)\n",
    "    labels_tens = torch.Tensor(meta_labels)\n",
    "\n",
    "    with open('/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v27/stats', 'rb') as f:\n",
    "        stats = torch.load(f)\n",
    "    stats = stats.int()\n",
    "    stats[0] += 1;                  stats[1] += batch_white_wins\n",
    "    stats[2] += batch_black_wins;   stats[3] += batch_draws\n",
    "    torch.save(stats, '/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v27/stats')\n",
    "\n",
    "    new_batch_index = stats[0]\n",
    "    torch.save(inputs_tens, '/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v27/inputs_{}'.format(new_batch_index))\n",
    "    torch.save(labels_tens, '/Users/Philip/Desktop/Projects/RL Chess/MCTS/Game Saves v27/labels_{}'.format(new_batch_index))\n",
    "\n",
    "    print('local batches: {} --  w: {}, b: {}, d: {}'.format(batch_count, white_wins, black_wins, draws))\n",
    "\n",
    "    batch_count += 1\n",
    "            \n",
    "    if batch_count % 5 == 0:\n",
    "        print(' -- global batches = {} --  w: {}, b: {}, d: {} (total: {})'.format(\n",
    "            new_batch_index, stats[1], stats[2], stats[3], stats[1] + stats[2] + stats[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
