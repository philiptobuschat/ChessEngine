{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Resources.Model import Model_v4\n",
    "from Resources.Game import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.98\n",
    "\n",
    "error_hist = torch.load('./error_hist QL v4')\n",
    "game_count = 0\n",
    "draw_count = 0\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "model = Model_v4() # Conv large\n",
    "model.train()\n",
    "model.load_state_dict(torch.load('./Model Saves QL v4/model_1510_games'))\n",
    "white_wins = 0\n",
    "black_wins = 0\n",
    "draws = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/Philip/Desktop/GitHub/ChessEngine/Train Value Function/Q-Learning/ValueTrain_Q-Learning.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Q-Learning/ValueTrain_Q-Learning.ipynb#W2sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m moves:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Q-Learning/ValueTrain_Q-Learning.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     game\u001b[39m.\u001b[39mPlayMove(move)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Q-Learning/ValueTrain_Q-Learning.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     board_batch\u001b[39m.\u001b[39mappend(board_to_tensor(game\u001b[39m.\u001b[39;49mpieces))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Q-Learning/ValueTrain_Q-Learning.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     game\u001b[39m.\u001b[39mFlipBoard()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/Q-Learning/ValueTrain_Q-Learning.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mif\u001b[39;00m game\u001b[39m.\u001b[39mis_over():\n",
      "File \u001b[0;32m~/Desktop/GitHub/ChessEngine/Resources/Game.py:2127\u001b[0m, in \u001b[0;36mboard_to_tensor\u001b[0;34m(board)\u001b[0m\n\u001b[1;32m   2125\u001b[0m     tensor[i, :] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy((board \u001b[39m==\u001b[39m i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m   2126\u001b[0m     \u001b[39m# black pieces channel\u001b[39;00m\n\u001b[0;32m-> 2127\u001b[0m     tensor[i\u001b[39m+\u001b[39m\u001b[39m6\u001b[39m, :] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy((board \u001b[39m==\u001b[39m i\u001b[39m+\u001b[39m\u001b[39m11\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m   2129\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i_limit = 1000\n",
    "while True:\n",
    "    t0 = time.time()\n",
    "\n",
    "    game = Game()\n",
    "    next_root = None\n",
    "    i = 0\n",
    "    boards_white = [];  boards_black = []\n",
    "    mat_white = [];     mat_black = []\n",
    "    value_white = [];   value_black = []\n",
    "\n",
    "    current_boards = []; current_labels = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    while not game.is_over():\n",
    "        i += 1\n",
    "        if i > i_limit:\n",
    "            break\n",
    "\n",
    "        moves = game.PossibleMoves()\n",
    "\n",
    "        game_ini = game.copy()\n",
    "        current_boards.append(board_to_tensor(game.pieces))\n",
    "        board_batch = [board_to_tensor(game.pieces)]\n",
    "\n",
    "        mate = False\n",
    "\n",
    "        for move in moves:\n",
    "            game.PlayMove(move)\n",
    "            board_batch.append(board_to_tensor(game.pieces))\n",
    "            game.FlipBoard()\n",
    "            if game.is_over():\n",
    "                mate = True\n",
    "                chosen_move = move\n",
    "                game = game_ini.copy()\n",
    "                break\n",
    "            game = game_ini.copy()\n",
    "\n",
    "\n",
    "        board_tensor = torch.stack(board_batch)\n",
    "        values = model(board_tensor)\n",
    "\n",
    "        if i > 2: # both sides played one move, start filling up labels\n",
    "            current_labels.append( max([values[i] for i in range(1, len(values))]) )\n",
    "\n",
    "        if not mate:\n",
    "\n",
    "            values_diff = [values[i] for i in range(1, len(values))]\n",
    "            move_prob = torch.softmax(torch.Tensor(values_diff), dim=0).numpy()\n",
    "            chosen_i = np.random.choice(range(len(moves)), p=move_prob)\n",
    "            chosen_move = moves[chosen_i]\n",
    "            \n",
    "        game.PlayMove(chosen_move)\n",
    "\n",
    "        mat_diff = game.MaterialDiff()\n",
    "\n",
    "        if i % 2 == 1:\n",
    "            #boards_white.append(board_to_tensor(game.pieces))\n",
    "            mat_white.append(mat_diff)\n",
    "            value_white.append(values[0].detach().item())\n",
    "        if i % 2 == 0:\n",
    "            #boards_black.append(board_to_tensor(game.pieces))\n",
    "            mat_black.append(mat_diff)\n",
    "            value_black.append(values[0].detach().item())\n",
    "\n",
    "        game.FlipBoard()\n",
    "\n",
    "    # plt.plot(model(torch.stack(boards_white)).detach().numpy())\n",
    "    # plt.title('values white')\n",
    "    # plt.show()\n",
    "    # plt.plot(model(torch.stack(boards_black)).detach().numpy())\n",
    "    # plt.title('values black')\n",
    "    # plt.show()\n",
    "\n",
    "    current_labels.append(-1)\n",
    "    current_labels.append(1)\n",
    "\n",
    "    if i <= i_limit: # game actually ended\n",
    "\n",
    "        winner = game.get_winner()\n",
    "        if winner == 'draw':\n",
    "            draws += 1;         reward_white = 0;   reward_black = 0\n",
    "        if winner == 'white':\n",
    "            white_wins += 1;    reward_white = 1;   reward_black = -1\n",
    "        if winner == 'black':\n",
    "            black_wins += 1;    reward_white = -1;  reward_black = 1\n",
    "\n",
    "    if i > i_limit:\n",
    "        print('game termianted')\n",
    "        continue\n",
    "\n",
    "    game_time = time.time() - t0\n",
    "\n",
    "    inputs_tens = torch.stack(current_boards)\n",
    "    labels_tens = torch.Tensor(current_labels)   \n",
    "\n",
    "    if winner != 'draw':\n",
    "        # newest training error\n",
    "        out = model(inputs_tens)\n",
    "        out = out.view(out.shape[0])\n",
    "        loss = criterion(out, labels_tens)\n",
    "        error_hist.append(loss.item())\n",
    "    else:\n",
    "        draw_count += 1\n",
    "        continue\n",
    "\n",
    "    if game_count % 10 == 0:\n",
    "        print(' -- {} -- winner: {}, i: {}   '.format(game_count, winner, i), 'wins: w = {}, b = {}, d = {}'.format(white_wins, black_wins, draws))\n",
    "\n",
    "\n",
    "    if game_count % 100 == 1:# or True:\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(mat_white, label='mat white')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(value_white, label='value white')\n",
    "        plt.plot(value_black, label='value black')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    torch.save(inputs_tens, './Game Saves QL v4/inputs_{}'.format(game_count))\n",
    "    torch.save(labels_tens, './Game Saves QL v4/labels_{}'.format(game_count))\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    if game_count > 5:\n",
    "        # sample some of previous game for replay. More recent -> more likely\n",
    "        decay_rate = 0.003\n",
    "        prob =  np.exp(-decay_rate * np.arange(game_count)) \n",
    "        prob = prob /sum(prob)\n",
    "        num_samples = 3\n",
    "        samples = np.random.choice(np.arange(game_count, 0, -1), size=num_samples, p=prob)\n",
    "\n",
    "        for indices in samples:\n",
    "            inputs_load = torch.load('./Game Saves QL v4/inputs_{}'.format(indices))\n",
    "            inputs_tens = torch.cat((inputs_load, inputs_tens))\n",
    "            labels_tens = torch.cat((torch.load('./Game Saves QL v4/labels_{}'.format(indices)), labels_tens))\n",
    "\n",
    "    len_boards = len(inputs_tens)\n",
    "    for c in range(len_boards):\n",
    "        tens = inputs_tens[c]\n",
    "        # if no pawns are on the board: add all other 3 rotated versions of the board to the current data set\n",
    "        if torch.sum(inputs_tens[c][0]) + torch.sum(inputs_tens[c][6]) == 0:\n",
    "            board_new = tensor_to_board(tens)\n",
    "            for _ in range(3):\n",
    "                board_new = rotate_board(board_new, 1)\n",
    "                tens_new = torch.stack([board_to_tensor(board_new)])\n",
    "                inputs_tens = torch.cat((inputs_tens, tens_new), 0)\n",
    "                labels_tens = torch.cat((labels_tens, torch.stack([labels_tens[c]])), 0)\n",
    "    \n",
    "    model.train()\n",
    "    # learning_rate = 5e-4 * 500 / (500 + game_count) # reducing learning rate, 1/n one possible options\n",
    "    learning_rate = 1e-5\n",
    "    # weight_decay = 1e-2 # regularization to avoid overfitting\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    random_order = [i for i in range(len(inputs_tens))]\n",
    "    np.random.shuffle(random_order)\n",
    "    for c in random_order:\n",
    "\n",
    "        inp = torch.stack([inputs_tens[c], mirror_board_tensor(inputs_tens[c])])\n",
    "        label = torch.stack([labels_tens[c], labels_tens[c]])\n",
    "\n",
    "        out = model(inp)\n",
    "        out = out.view(out.shape[0])\n",
    "\n",
    "        loss = criterion(out, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "    torch.save(error_hist, './error_hist QL v4')\n",
    "\n",
    "    if game_count % 100  == 1:\n",
    "        interval = 50\n",
    "        plt.figure()\n",
    "        plt.plot([np.mean(error_hist[i:i+interval]) for i in range(len(error_hist) - interval)])\n",
    "        plt.show()\n",
    "\n",
    "    if game_count % 100 == 9:    \n",
    "        torch.save(model.state_dict(), './Model Saves QL v4/model_{}_games'.format(game_count+1))\n",
    "\n",
    "    game_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
