{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Resources.Model import Model_v25\n",
    "from Resources.Game import *\n",
    "from Resources.TS_ModelGuided import *\n",
    "from Resources.TS_ModelGuided_MCRollout import *\n",
    "from Resources.TS_ModelGuided_SensAnalysis import *\n",
    "\n",
    "from test_games import test_games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tree Search guided by value function and sensitivity analysis of the value function\n",
    "\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import torch\n",
    "# import torch.autograd.functional as Func\n",
    "# import sys, os\n",
    "\n",
    "# project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "# sys.path.append(project_root)\n",
    "\n",
    "# from Resources.Game import *\n",
    "# from Resources.RollingMedian import RollingMedian\n",
    "\n",
    "# def sensitivity_extraction_1(grad):\n",
    "#     '''\n",
    "#     extract scalar value from model gradient.\n",
    "#     R^(12*8*8) -> R\n",
    "#     method 1: 2-norm of gradient\n",
    "#     '''\n",
    "#     return torch.norm(grad, p=2).item()\n",
    "\n",
    "# def sensitivity_extraction_2(grad):\n",
    "#     '''\n",
    "#     extract scalar value from model gradient.\n",
    "#     R^(12*8*8) -> R\n",
    "#     method 2: unweighted sum of gradient\n",
    "#     '''\n",
    "#     return torch.sum(grad).item()\n",
    "\n",
    "# def sensitivity_extraction_3(grad):\n",
    "#     '''\n",
    "#     extract scalar value from model gradient.\n",
    "#     R^(12*8*8) -> R\n",
    "#     method 3: activation function -> sum of gradient\n",
    "#     '''\n",
    "#     return torch.sum(torch.tanh(grad)).item()\n",
    "\n",
    "# def sensitivity_extraction_4(grad):\n",
    "#     '''\n",
    "#     extract scalar value from model gradient.\n",
    "#     R^(12*8*8) -> R\n",
    "#     method 4: pooling over each piece type -> act. function -> then sum\n",
    "#     '''\n",
    "#     return torch.sum(torch.tanh(torch.sum(grad, dim=(1, 2)))).item()\n",
    "\n",
    "# def sensitivity_extraction_5(grad):\n",
    "#     '''\n",
    "#     extract scalar value from model gradient.\n",
    "#     R^(12*8*8) -> R\n",
    "#     method 5: learned extraction function (e.g. NN)\n",
    "\n",
    "#     possible future implementation\n",
    "#     '''\n",
    "#     return None\n",
    "\n",
    "# def SensAnalysis_TS(game, model, root=None, tmax=60, batches=10, prints=False, factors=None, factor_wins=1, \n",
    "#                    factor_mat=0, factor_value_sum=0.5, factor_value_indi=0.5, \n",
    "#                    factor_grad_sum=0.1, factor_grad_indi=0.1,\n",
    "#                    factor_explore=1e-3,\n",
    "#                    sensitivity_extraction = sensitivity_extraction_1):\n",
    "#     '''\n",
    "#     Tree Search Guided by model and its sensitivity via model gradient\n",
    "#     '''\n",
    "\n",
    "#     # Create the root node if not passed\n",
    "#     if root is None:\n",
    "#         if factors is None:\n",
    "#             root = SensAnalysis_Node(None, None, game, [factor_wins, factor_mat, \n",
    "#                                 factor_value_sum, factor_value_indi, \n",
    "#                                 factor_grad_sum, factor_grad_indi,\n",
    "#                                 factor_explore]) \n",
    "#         if factors is not None:\n",
    "#             root = SensAnalysis_Node(None, None, game, factors) \n",
    "\n",
    "#     root.game.FlipBoard()\n",
    "#     board_batch = [board_to_tensor(root.game.pieces)]\n",
    "#     nodes = [root]\n",
    "#     root.game.FlipBoard()\n",
    "\n",
    "#     for _ in range(batches):\n",
    "\n",
    "#         t0 = time.time()\n",
    "#         while time.time() - t0 < float(tmax) / float(batches):\n",
    "\n",
    "#             node = root\n",
    "\n",
    "#             # Selection\n",
    "#             while not node.is_leaf() and node.is_fully_expanded():\n",
    "#                 node = node.select_child()\n",
    "\n",
    "#             # Expansion\n",
    "#             if not node.is_fully_expanded():\n",
    "#                 node, board_batch = node.expand(board_batch)\n",
    "\n",
    "#             if node.game.is_over():\n",
    "#                 winner = node.game.get_winner()\n",
    "#             else:\n",
    "#                 winner = None\n",
    "\n",
    "#             # backpropagation of winner, matdiff\n",
    "#             for_side = node.player\n",
    "\n",
    "#             node.backpropagate(winner, node.matdiff, for_side, prints)\n",
    "            \n",
    "#             nodes.append(node)\n",
    "#             # board_batch.append(board_to_tensor(node.game.pieces))\n",
    "\n",
    "#         batch_size = len(board_batch)\n",
    "\n",
    "#         if batch_size == 0:\n",
    "#             continue\n",
    "\n",
    "#         # get model values\n",
    "#         tens_board_batch = torch.stack(board_batch)\n",
    "#         tens_board_batch.requires_grad = True\n",
    "#         values = model(tens_board_batch).detach().numpy()\n",
    "\n",
    "#         # get each gradient w.r.t. input by vectorising input and using jacobian functionality\n",
    "#         vectorized_input = tens_board_batch.view(batch_size, -1)\n",
    "#         jacobian_matrix = Func.jacobian(lambda x: model(x.view(-1, 12, 8, 8)).sum(), vectorized_input)\n",
    "#         gradients = jacobian_matrix.view(batch_size, 12, 8, 8)\n",
    "\n",
    "#         for node, value, grad in zip(nodes, values, gradients):\n",
    "\n",
    "#             for_side = node.player\n",
    "#             grad_ext = sensitivity_extraction(grad)\n",
    "\n",
    "#             # node.backpropagate_value(value, for_side)\n",
    "#             node.backpropagate_value_grad(value, grad_ext, for_side)\n",
    "#             node.value = value\n",
    "#             node.grad = grad_ext\n",
    "\n",
    "#         # for node, value in zip(nodes, values):\n",
    "\n",
    "#         #     for_side = node.player\n",
    "#         #     node.backpropagate_value(value, for_side)\n",
    "#         #     node.value = value\n",
    "\n",
    "#         board_batch = []\n",
    "#         nodes = []\n",
    "\n",
    "#     # Choose the best move based on the visit counts of child nodes\n",
    "#     best_move = root.get_best_move()\n",
    "\n",
    "#     return best_move, root\n",
    "\n",
    "\n",
    "# class SensAnalysis_Node:\n",
    "#     '''\n",
    "#     every instance corresponds to a move in the expansion tree.\n",
    "#     game is the state after playing the move\n",
    "#     '''\n",
    "#     def __init__(self, move, parent, game, factors):\n",
    "#         self.move = move;   self.parent = parent\n",
    "#         self.children = []; self.wins = 0\n",
    "#         self.value = None; self.grad = None\n",
    "#         self.visits = 0 ;   self.game = game\n",
    "\n",
    "#         self.value_rollmed = RollingMedian()\n",
    "#         self.grad_rollmed = RollingMedian()\n",
    "#         self.matdiff_rollmed = RollingMedian()\n",
    "\n",
    "#         self.factors = factors\n",
    "\n",
    "#         # game.turn is after our move if played and the board is flipped, player should be before our move \n",
    "#         if game.turn == 'white': self.player = 'black'\n",
    "#         if game.turn == 'black': self.player = 'white'\n",
    "\n",
    "#         # next child nodes\n",
    "#         self.untried_moves = game.PossibleMoves()\n",
    "\n",
    "#         self.matdiff = - self.game.MaterialDiff() # minus since board is flipped after move already\n",
    "\n",
    "#     def is_leaf(self):\n",
    "#         return len(self.children) == 0\n",
    "\n",
    "#     def is_fully_expanded(self):\n",
    "#         return len(self.untried_moves) == 0\n",
    "\n",
    "#     def select_child(self):\n",
    "\n",
    "#         # for child in self.children:\n",
    "#         #     print(child.value)\n",
    "#         #     print(child.value_rollmed.get_median())\n",
    "\n",
    "#         self.values_win = [child.wins / max(child.visits, 1)\n",
    "#                         for child in self.children]\n",
    "        \n",
    "#         self.values_mat = [np.tanh(child.matdiff_rollmed.get_median() + self.matdiff)    \n",
    "#                         for child in self.children]\n",
    "        \n",
    "#         self.values_value_sum = [np.tanh(child.value_rollmed.get_median() + self.value)\n",
    "#                                 if (child.value_rollmed.get_median() is not None and self.value is not None) else 0\n",
    "#                         for child in self.children]\n",
    "        \n",
    "#         self.values_value_indi = [child.value + self.value \n",
    "#                                   if (child.value is not None and self.value is not None) else 0\n",
    "#                         for child in self.children]\n",
    "        \n",
    "#         self.values_grad_sum = [np.tanh(child.grad_rollmed.get_median())\n",
    "#                                  if (child.grad_rollmed.get_median() is not None) else 0\n",
    "#                         for child in self.children]\n",
    "        \n",
    "#         self.values_grad_indi = [child.grad \n",
    "#                                   if (child.value is not None) else 0\n",
    "#                         for child in self.children]\n",
    "        \n",
    "#         # self.values_explore = [(self.visits) / max(child.visits, 1)\n",
    "#         #                 for child in self.children]\n",
    "\n",
    "#         self.values_explore = [-np.log10(max(child.visits, 1))\n",
    "#                         for child in self.children]\n",
    "        \n",
    "\n",
    "#         self.children_values = [\n",
    "#               self.factors[0]   * self.values_win[c] \n",
    "#             + self.factors[1]   * self.values_mat[c]\n",
    "#             + self.factors[2]   * self.values_value_sum[c]\n",
    "#             + self.factors[3]   * self.values_value_indi[c]\n",
    "#             + self.factors[4]   * self.values_grad_sum[c]\n",
    "#             + self.factors[5]   * self.values_grad_indi[c]\n",
    "#             + self.factors[6]   * self.values_explore[c]\n",
    "#             for c, child in enumerate(self.children)\n",
    "#         ]\n",
    "\n",
    "#         # select child with highest ucb value or random one of the best ones\n",
    "#         max_value = max(self.children_values)\n",
    "#         max_indices = [i for i, value in enumerate(self.children_values) if value == max_value]\n",
    "#         selected_index = random.choice(max_indices)\n",
    "\n",
    "#         return self.children[selected_index]\n",
    "\n",
    "#     def expand(self, board_batch):\n",
    "#         # Choose an untried move and create a new child node\n",
    "#         move = random.choice(self.untried_moves)\n",
    "#         self.untried_moves.remove(move)\n",
    "#         child_game = self.game.copy()\n",
    "#         child_game.PlayMove(move)\n",
    "#         board_batch.append(board_to_tensor(child_game.pieces))\n",
    "#         child_game.FlipBoard()\n",
    "#         child_node = SensAnalysis_Node(move, self, child_game, self.factors)\n",
    "#         self.children.append(child_node)\n",
    "#         return child_node, board_batch\n",
    "\n",
    "#     def backpropagate_value(self, value, for_side):\n",
    "#         ''' \n",
    "#         backpropagate only value of leaf through tree\n",
    "#         self.player is from whos perspective a node is\n",
    "#         '''\n",
    "\n",
    "#         # value (applies independent of outcome)\n",
    "#         if for_side == self.player:\n",
    "#             self.value_rollmed.add_number(value)\n",
    "#         else:\n",
    "#             self.value_rollmed.add_number(-value)\n",
    "\n",
    "#         if self.parent is not None:\n",
    "#             self.parent.backpropagate_value(value, for_side)\n",
    "\n",
    "#     def backpropagate_value_grad(self, value, grad, for_side):\n",
    "#         ''' \n",
    "#         backpropagate only value of leaf through tree\n",
    "#         self.player is from whos perspective a node is\n",
    "#         '''\n",
    "\n",
    "#         # value (applies independent of outcome)\n",
    "#         if for_side == self.player:\n",
    "#             self.value_rollmed.add_number(value)\n",
    "#             self.grad_rollmed.add_number(grad)\n",
    "#         else:\n",
    "#             self.value_rollmed.add_number(-value)\n",
    "#             self.grad_rollmed.add_number(-grad)\n",
    "\n",
    "#         if self.parent is not None:\n",
    "#             self.parent.backpropagate_value_grad(value, grad, for_side)\n",
    "\n",
    "#     def backpropagate(self, winner, matdiff, for_side, prints):\n",
    "#         ''' \n",
    "#         main function backpropagation\n",
    "#         after leaf reached: backpropagate winner and matdiff of leaf through tree\n",
    "#         self.player is from whos perspective a node is\n",
    "#         '''\n",
    "#         if prints:\n",
    "#             print('backprob curr move: ', self.move)\n",
    "#         self.visits += 1\n",
    "\n",
    "#         # winner (if decisive, i.e. game terminates and no draw)\n",
    "#         if winner is not None:\n",
    "#             if winner == self.player:\n",
    "#                 self.wins += 1\n",
    "#             elif winner == self.game.opponent[self.player]:\n",
    "#                 self.wins -= 1\n",
    "\n",
    "#         # matdiff (applies independent of outcome)\n",
    "#         if for_side == self.player:\n",
    "#             self.matdiff_rollmed.add_number(matdiff)\n",
    "#         else:\n",
    "#             self.matdiff_rollmed.add_number(-matdiff)\n",
    "\n",
    "#         if self.parent is not None:\n",
    "#             self.parent.backpropagate(winner, matdiff, for_side, prints)\n",
    "\n",
    "#     def get_best_move(self):\n",
    "#         best_child = None\n",
    "#         highest_visit_count = 0\n",
    "        \n",
    "#         # Iterate through the children and find the best one\n",
    "#         for child in self.children:\n",
    "#             if child.visits > highest_visit_count:\n",
    "#                 best_child = child\n",
    "#                 highest_visit_count = child.visits\n",
    "        \n",
    "#         if best_child is None:\n",
    "#             choice = np.random.choice([i for i in range(len(self.children))])\n",
    "#             return [child.move for child in self.children][choice]\n",
    "        \n",
    "#         return best_child.move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model_v25()\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load('../Train Value Function/Monte Carlo/Model Saves MC v25/model_2000_batches'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factors_short_to_long(short):\n",
    "    # set parameters for material and gradients to zero\n",
    "    return [short[0], 0.0, short[1], short[2], 0.0, 0.0, short[3]]\n",
    "\n",
    "def factors_mid_to_long(mid):\n",
    "    # set parameter for material to zero\n",
    "    return [mid[0], 0.0, mid[1], mid[2], mid[3], mid[4], mid[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factors:\n",
    "# factor_wins=1, factor_mat=0, factor_value_sum=0.5, factor_value_indi=0.5, \n",
    "# factor_grad_sum=0.1, factor_grad_indi=0.1, factor_explore=1e-3,\n",
    "\n",
    "factors_1 = [1, 0, 0.5, 0.5, 0.0, 0.0, 0.0001]\n",
    "factors_2 = [1, 0, 0.5, 0.5, 0.0, 0.0, 10]\n",
    "\n",
    "factors = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0] # factor wins\n",
    "\n",
    "# when using only [wins, value_sum, value_indi, explore]\n",
    "# --> [0.23509465 0.97146549 0.01327048 0.02844645]\n",
    "\n",
    "n_games = 3\n",
    "tmax = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]\n"
     ]
    }
   ],
   "source": [
    "# Small RL loop, pseudo SGD\n",
    "factors = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "factors = np.array(factors)\n",
    "factors = 1/np.linalg.norm(factors) * factors \n",
    "print(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]\n",
      "1 [0.37544934 0.41299428 0.41299428 0.4542937  0.41299428 0.37544934]\n",
      "2 [0.33234974 0.40214319 0.44235751 0.48659326 0.40214319 0.36558472]\n",
      "3 [0.32142085 0.38891922 0.47059226 0.47059226 0.38891922 0.38891922]\n",
      "4 [0.29186175 0.38846799 0.51705089 0.47004627 0.35315272 0.38846799]\n",
      "5 [0.28531769 0.37975785 0.5054577  0.5054577  0.34523441 0.37975785]\n",
      "6 [0.25376023 0.33775487 0.4945069  0.54395759 0.33775487 0.40868339]\n",
      "7 [0.24965597 0.3322921  0.48650886 0.58867573 0.3322921  0.36552131]\n",
      "8 [0.25208305 0.30502049 0.49123855 0.59439864 0.33552254 0.36907479]\n",
      "9 [0.23564215 0.31363971 0.50511989 0.61119506 0.31363971 0.34500368]\n",
      "10 [0.25753363 0.34277727 0.45623654 0.60725084 0.3116157  0.37705499]\n",
      "11 [0.24531325 0.35916312 0.43458738 0.63627938 0.29682903 0.35916312]\n",
      "12 [0.25006973 0.36612709 0.44301378 0.64861648 0.2750767  0.33284281]\n",
      "13 [0.25732896 0.34250485 0.45587396 0.66744506 0.25732896 0.31136805]\n",
      "14 [0.25937423 0.3452271  0.41772479 0.67274996 0.25937423 0.3452271 ]\n",
      "15 [0.24238457 0.35487525 0.39036278 0.69155148 0.24238457 0.35487525]\n",
      "16 [0.25085158 0.36727179 0.36727179 0.71570883 0.25085158 0.30353041]\n",
      "17 [0.25527178 0.33976674 0.37374342 0.72832019 0.23206526 0.30887886]\n",
      "18 [0.29071703 0.3517676  0.38694436 0.68549554 0.26428821 0.31978873]\n",
      "19 [0.27889718 0.33746559 0.37121215 0.72338746 0.23049354 0.3067869 ]\n",
      "20 [0.24435371 0.29566799 0.35775827 0.76688663 0.22213974 0.29566799]\n",
      "21 [0.26783185 0.32407654 0.3564842  0.76415554 0.20122604 0.26783185]\n",
      "22 [0.25675779 0.31067693 0.31067693 0.80581594 0.19290593 0.23341617]\n",
      "23 [0.23940996 0.28968605 0.31865466 0.82650812 0.16352023 0.21764542]\n",
      "24 [0.25567686 0.309369   0.3403059  0.80242352 0.15875522 0.23243351]\n",
      "25 [0.23243351 0.309369   0.3403059  0.80242352 0.15875522 0.25567686]\n",
      "26 [0.25437438 0.307793   0.307793   0.79833576 0.17374112 0.27981181]\n",
      "27 [0.28774021 0.31651423 0.34816566 0.746324   0.19653044 0.31651423]\n",
      "28 [0.28774021 0.31651423 0.31651423 0.746324   0.19653044 0.34816566]\n",
      "29 [0.2705271  0.3273378  0.29757981 0.7718454  0.16797605 0.3273378 ]\n",
      "30 [0.27594208 0.33388992 0.30353629 0.78729496 0.15576211 0.27594208]\n",
      "31 [0.27925111 0.30717622 0.30717622 0.79673602 0.14329998 0.27925111]\n",
      "32 [0.24028669 0.2907469  0.2907469  0.82953483 0.12330507 0.26431536]\n",
      "33 [0.26043906 0.28648297 0.28648297 0.81736935 0.13364642 0.28648297]\n",
      "34 [0.2195452  0.29221467 0.29221467 0.83372255 0.12392754 0.2656497 ]\n",
      "35 [0.21760287 0.31859236 0.28962942 0.82634653 0.12283115 0.26329947]\n",
      "36 [0.22085221 0.32334971 0.26723117 0.83868588 0.1133321  0.24293743]\n",
      "37 [0.24023358 0.3197509  0.26425694 0.82935148 0.11207074 0.26425694]\n",
      "38 [0.2050467  0.30020887 0.24810651 0.85653095 0.10522138 0.24810651]\n",
      "39 [0.21921458 0.32095207 0.29177461 0.83246701 0.11249174 0.24113604]\n",
      "40 [0.22085221 0.32334971 0.26723117 0.83868588 0.1133321  0.24293743]\n",
      "41 [0.23536935 0.34460426 0.3132766  0.81255882 0.10980154 0.23536935]\n",
      "42 [0.22341147 0.32709673 0.27032788 0.84840468 0.09474827 0.20310133]\n",
      "43 [0.221887   0.32486475 0.29533159 0.8426155  0.08554704 0.20171545]\n",
      "44 [0.24002481 0.35142032 0.29043002 0.82863074 0.09253995 0.19836761]\n",
      "45 [0.25790088 0.34326607 0.31206006 0.80940343 0.09039268 0.23445534]\n",
      "46 [0.25891578 0.3446169  0.31328809 0.81258862 0.09982324 0.21397998]\n",
      "47 [0.26035115 0.31502489 0.31502489 0.81709343 0.10037664 0.23668286]\n",
      "48 [0.27655582 0.33463254 0.33463254 0.78904603 0.11728667 0.25141438]\n",
      "49 [0.28407335 0.31248068 0.31248068 0.81049441 0.10952257 0.23477136]\n",
      "50 [0.28901333 0.28901333 0.28901333 0.82458877 0.11142715 0.23885399]\n",
      "51 [0.28937759 0.28937759 0.26307054 0.82562805 0.11156759 0.26307054]\n",
      "52 [0.29145007 0.29145007 0.24086783 0.83154106 0.10215147 0.26495461]\n",
      "53 [0.29889227 0.24701841 0.22456219 0.85277453 0.10475992 0.24701841]\n",
      "54 [0.30048758 0.22576076 0.22576076 0.85732612 0.10531906 0.24833684]\n",
      "55 [0.29573022 0.24440514 0.24440514 0.84375283 0.09422876 0.26884566]\n",
      "56 [0.31588839 0.26106478 0.26106478 0.81933313 0.10065178 0.28717126]\n",
      "57 [0.34353504 0.25810296 0.25810296 0.81003765 0.10946085 0.28391325]\n",
      "58 [0.31246689 0.2582371  0.2582371  0.81045863 0.12046951 0.31246689]\n",
      "59 [0.36681965 0.25054276 0.27559703 0.7863105  0.12856805 0.30315674]\n",
      "60 [0.40425915 0.25101312 0.25101312 0.78778671 0.11709947 0.27611444]\n",
      "61 [0.37710192 0.23415062 0.23415062 0.80835146 0.12015629 0.28332225]\n",
      "62 [0.42350401 0.26296267 0.26296267 0.75026319 0.13494143 0.31818483]\n",
      "63 [0.41612454 0.2583806  0.28421866 0.73719    0.1325901  0.34390458]\n",
      "64 [0.42419427 0.23944661 0.2897304  0.75148603 0.11170361 0.31870344]\n",
      "65 [0.43064729 0.22099015 0.29413789 0.76291794 0.10309354 0.29413789]\n",
      "66 [0.47204611 0.220213   0.26645772 0.76023499 0.09339181 0.26645772]\n",
      "67 [0.4745418  0.22137725 0.24351498 0.76425431 0.10327412 0.26786647]\n",
      "68 [0.49996947 0.23323945 0.25656339 0.7320053  0.11968872 0.28221973]\n",
      "69 [0.46957756 0.19914673 0.26506429 0.75625936 0.11241313 0.29157072]\n",
      "70 [0.43343004 0.20219831 0.24465996 0.76784775 0.12554924 0.3256424 ]\n",
      "71 [0.41224686 0.1923162  0.2327026  0.8033525  0.11941323 0.28157015]\n",
      "72 [0.43497599 0.18447228 0.24553261 0.7705865  0.12599705 0.3268039 ]\n",
      "73 [0.41829381 0.21465087 0.25972755 0.74103301 0.1332813  0.3802671 ]\n",
      "74 [0.40412507 0.22811806 0.30362514 0.7159322  0.1287667  0.40412507]\n",
      "75 [0.37377533 0.21098643 0.30890523 0.72838238 0.14410657 0.41115287]\n",
      "76 [0.43548814 0.22347427 0.29744426 0.701358   0.15263594 0.39589831]\n",
      "77 [0.48908131 0.22816004 0.30368101 0.65096722 0.17142001 0.40419943]\n",
      "78 [0.53641025 0.22749031 0.3027896  0.59005128 0.15537894 0.44331426]\n",
      "79 [0.54072946 0.22932208 0.27747971 0.5948024  0.15663006 0.44688385]\n",
      "80 [0.53000332 0.22477315 0.32909036 0.58300365 0.16887539 0.43801927]\n",
      "81 [0.55273646 0.21310383 0.31200532 0.55273646 0.19373076 0.45680699]\n",
      "82 [0.54517213 0.21018746 0.30773545 0.59968934 0.19107951 0.40959589]\n",
      "83 [0.54690767 0.21085658 0.30871512 0.60159844 0.17426164 0.41089983]\n",
      "84 [0.55516531 0.19458205 0.28488759 0.61068184 0.16081162 0.41710392]\n",
      "85 [0.57397843 0.16626111 0.26776518 0.63137628 0.13740587 0.39203499]\n",
      "86 [0.60862306 0.14569952 0.28392715 0.60862306 0.12041283 0.37790703]\n",
      "87 [0.62159212 0.14880421 0.26361574 0.62159212 0.11179881 0.35087255]\n",
      "88 [0.60769859 0.14547821 0.28349588 0.60769859 0.13225292 0.37733301]\n",
      "89 [0.64506452 0.12762258 0.27357032 0.58642229 0.12762258 0.3641221 ]\n",
      "90 [0.59396524 0.12926414 0.30479809 0.59396524 0.14219056 0.40568625]\n",
      "91 [0.63714432 0.11459601 0.27021139 0.57922211 0.12605561 0.39561649]\n",
      "92 [0.61609138 0.1218904  0.31615229 0.56008307 0.1218904  0.4207987 ]\n",
      "93 [0.64920136 0.12844103 0.30285723 0.53653005 0.14128513 0.40310297]\n",
      "94 [0.60974733 0.12063526 0.28445163 0.60974733 0.13269878 0.37860512]\n",
      "95 [0.61925124 0.12251556 0.28888527 0.61925124 0.11137778 0.34955118]\n",
      "96 [0.59674601 0.11806302 0.30622506 0.65642061 0.10733002 0.30622506]\n",
      "97 [0.59010929 0.12842498 0.30281937 0.64912022 0.10613634 0.33310131]\n",
      "98 [0.60166206 0.11903563 0.28067979 0.66182826 0.10821421 0.30874777]\n",
      "99 [0.5757641  0.10355624 0.2685982  0.69667457 0.11391186 0.29545802]\n",
      "100 [0.56825419 0.11242606 0.26509477 0.68758757 0.13603554 0.32076468]\n",
      "101 [0.57191297 0.12446492 0.24254693 0.69201469 0.12446492 0.32282996]\n",
      "102 [0.51060013 0.12223361 0.2165443  0.74756965 0.13445697 0.31704251]\n",
      "103 [0.50499751 0.10990217 0.21416824 0.73936685 0.14627979 0.34492009]\n",
      "104 [0.50701962 0.11034224 0.19547801 0.74232743 0.14686552 0.34630123]\n",
      "105 [0.5091265  0.11080076 0.21591934 0.74541211 0.16222339 0.3161275 ]\n",
      "106 [0.49540294 0.10781411 0.23110913 0.72531945 0.15785064 0.37220356]\n",
      "107 [0.52027389 0.10293341 0.24271161 0.69248454 0.16577529 0.39088947]\n",
      "108 [0.52860905 0.10458248 0.24660002 0.70357864 0.15311921 0.36104709]\n",
      "109 [0.53676536 0.08776542 0.250405   0.71443469 0.14134709 0.33328905]\n",
      "110 [0.50727884 0.08294414 0.21513575 0.74270695 0.13358237 0.34647827]\n",
      "111 [0.50129503 0.07451431 0.21259803 0.73394605 0.13200665 0.37663037]\n",
      "112 [0.49388561 0.08075424 0.25344141 0.72309792 0.15736718 0.37106357]\n",
      "113 [0.45907472 0.07506238 0.25913571 0.73934442 0.16090289 0.37940059]\n",
      "114 [0.44042147 0.07921367 0.24860644 0.7802335  0.14033185 0.33089517]\n",
      "115 [0.46254865 0.08319344 0.23736059 0.74493922 0.14738225 0.38227161]\n",
      "116 [0.39640009 0.07129604 0.24613327 0.77247163 0.15282939 0.39640009]\n",
      "117 [0.45983639 0.08270562 0.25956565 0.7405711  0.14651804 0.38003007]\n",
      "118 [0.43685335 0.07857192 0.22417484 0.77391236 0.15311443 0.36103583]\n",
      "119 [0.42058909 0.09153244 0.23741158 0.74509922 0.1621553  0.42058909]\n",
      "120 [0.40544414 0.08823646 0.20805695 0.79009593 0.15631627 0.36858558]\n",
      "121 [0.41280222 0.08983779 0.19257528 0.80443474 0.14468466 0.34115886]\n",
      "122 [0.41615432 0.0905673  0.21355297 0.81096704 0.13259959 0.3126629 ]\n",
      "123 [0.44450154 0.08794226 0.20736325 0.78746159 0.14163189 0.33396059]\n",
      "124 [0.50640812 0.10019015 0.19524225 0.74143214 0.16135723 0.34588356]\n",
      "125 [0.50450079 0.10979407 0.19450689 0.7386396  0.17682445 0.34458082]\n",
      "126 [0.46467159 0.11123868 0.17915101 0.74835824 0.17915101 0.3840261 ]\n",
      "127 [0.43710863 0.1266148  0.18537673 0.77436459 0.1685243  0.3612468 ]\n",
      "128 [0.40538064 0.12916677 0.17192097 0.78997219 0.17192097 0.36852786]\n",
      "129 [0.37888208 0.12072351 0.16068299 0.81216738 0.19442641 0.34443825]\n",
      "130 [0.3516462  0.10185938 0.14913232 0.82916336 0.1804501  0.3516462 ]\n",
      "131 [0.3323569  0.10589915 0.14095177 0.86204821 0.15504695 0.30214264]\n",
      "132 [0.33894641 0.0981807  0.13067851 0.87913969 0.14374636 0.25465545]\n",
      "133 [0.37075052 0.10739322 0.1299458  0.87421033 0.1299458  0.2302069 ]\n",
      "134 [0.37460149 0.08967662 0.13129554 0.88329072 0.13129554 0.1922298 ]\n",
      "135 [0.34277768 0.09026408 0.13215563 0.88907703 0.1453712  0.21283797]\n",
      "136 [0.37392528 0.08951474 0.13105853 0.88169625 0.14416438 0.19188279]\n",
      "137 [0.31880095 0.08395026 0.12291157 0.90957633 0.148723   0.1635953 ]\n",
      "138 [0.31668032 0.08339183 0.13430337 0.90352592 0.16250708 0.17875779]\n",
      "139 [0.34268659 0.08203645 0.13212052 0.88884076 0.19343765 0.17585241]\n",
      "140 [0.34356048 0.09047021 0.13245744 0.8911074  0.17630085 0.17630085]\n",
      "141 [0.31524516 0.0830139  0.12154066 0.89943125 0.17794768 0.19574244]\n",
      "142 [0.28763765 0.07574398 0.13418508 0.90273015 0.17860035 0.21610642]\n",
      "143 [0.33618864 0.08048089 0.1425768  0.87198676 0.18976972 0.2525835 ]\n",
      "144 [0.33551249 0.08031902 0.12935459 0.870233   0.20832686 0.2520755 ]\n",
      "145 [0.3384366  0.08912094 0.13048196 0.87781739 0.19103864 0.23115675]\n",
      "146 [0.30819624 0.09820082 0.14377582 0.87931985 0.19136562 0.25470764]\n",
      "147 [0.30544445 0.11776206 0.14249209 0.87146866 0.18965697 0.27767677]\n",
      "148 [0.32810709 0.12649949 0.16837082 0.8510253  0.22410156 0.27116289]\n",
      "149 [0.33104682 0.12763288 0.15443578 0.85865019 0.22610943 0.24872037]\n",
      "150 [0.38142685 0.14705656 0.16176222 0.81762233 0.23683607 0.28657164]\n",
      "151 [0.37418867 0.15869252 0.17456178 0.80210664 0.2555759  0.30924683]\n",
      "152 [0.36199833 0.18576238 0.18576238 0.77597556 0.29917217 0.32908939]\n",
      "153 [0.38794425 0.18097885 0.19907674 0.75599359 0.32061508 0.32061508]\n",
      "154 [0.3695243  0.1896244  0.17238581 0.79210816 0.27762908 0.30539199]\n",
      "155 [0.39700964 0.16837084 0.18520793 0.77365948 0.27116293 0.32810714]\n",
      "156 [0.36308721 0.16938286 0.18632115 0.77830969 0.30007208 0.33007928]\n",
      "157 [0.36069378 0.18509294 0.20360224 0.77317916 0.29809404 0.32790344]\n",
      "158 [0.36944775 0.18958511 0.20854363 0.79194407 0.25233779 0.30532872]\n",
      "159 [0.39009523 0.1819823  0.20018053 0.76018524 0.26644029 0.35463203]\n",
      "160 [0.36615133 0.15528391 0.1708123  0.7848779  0.25008629 0.36615133]\n",
      "161 [0.31622044 0.13410833 0.17849819 0.82019438 0.2375811  0.34784248]\n",
      "162 [0.3669199  0.14146351 0.17117084 0.7865254  0.25061123 0.3669199 ]\n",
      "163 [0.39266303 0.137626   0.16652746 0.76518917 0.24381285 0.39266303]\n",
      "164 [0.38022001 0.1612504  0.19511298 0.74094123 0.25969538 0.41824201]\n",
      "165 [0.38022001 0.1612504  0.19511298 0.74094123 0.25969538 0.41824201]\n",
      "166 [0.35155993 0.14909573 0.21829106 0.75359993 0.24012016 0.42538752]\n",
      "167 [0.3795598  0.14633674 0.21425161 0.73965468 0.25924445 0.41751578]\n",
      "168 [0.38266047 0.14753218 0.21600186 0.745697   0.28749847 0.38266047]\n"
     ]
    }
   ],
   "source": [
    "# small RL loop to find best factor hyperparameters \n",
    "\n",
    "factor_hist = []\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(len(factor_hist), factors)\n",
    "\n",
    "    # t0 = time.time()\n",
    "\n",
    "    for i in range(6):\n",
    "\n",
    "        # print(factors)\n",
    "        factors_test = factors.copy()\n",
    "        factors_test[i] *= 1.1\n",
    "\n",
    "        wins_1, draws, wins_2 = test_games(n_games, model, tmax, factors_mid_to_long(factors), \n",
    "                                        factors_mid_to_long(factors_test))\n",
    "        \n",
    "        if wins_1 > wins_2:\n",
    "            factors[i] *= 1/1.1\n",
    "        elif wins_2 > wins_1:\n",
    "            factors[i] *= 1.1\n",
    "\n",
    "    factors = 1/np.linalg.norm(factors) * factors \n",
    "    \n",
    "    factor_hist.append(factors.copy())\n",
    "\n",
    "    # print('duration: ', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_games import test_games\n",
    "\n",
    "out = test_games(1, model, 1, np.ones(7), np.ones(7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]\n",
      "[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/Philip/Desktop/GitHub/ChessEngine/Train Value Function/TreeSearchParameterLearning.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/TreeSearchParameterLearning.ipynb#X25sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     factors_test \u001b[39m=\u001b[39m factors\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/TreeSearchParameterLearning.ipynb#X25sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     factors_test[i] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/TreeSearchParameterLearning.ipynb#X25sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     arg_list \u001b[39m=\u001b[39m [n_games, model, tmax, factors_mid_to_long(factors), \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/TreeSearchParameterLearning.ipynb#X25sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m                                     factors_mid_to_long(factors_test)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/TreeSearchParameterLearning.ipynb#X25sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     process_list\u001b[39m.\u001b[39mappend(multiprocessing\u001b[39m.\u001b[39mProcess(target\u001b[39m=\u001b[39mrun_test_games, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/TreeSearchParameterLearning.ipynb#X25sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                                        args\u001b[39m=\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mProcess 1\u001b[39m\u001b[39m\"\u001b[39m, arg_list, output_queue)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Philip/Desktop/GitHub/ChessEngine/Train%20Value%20Function/TreeSearchParameterLearning.ipynb#X25sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mstart processes\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# small RL loop to find best factor hyperparameters \n",
    "# Threading to run test games in parallel\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from test_games import test_games, run_test_games\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(factors)\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    process_list = []\n",
    "\n",
    "    for i in range(2):\n",
    "\n",
    "        output_queue = multiprocessing.Queue()\n",
    "\n",
    "        # print(factors)\n",
    "        factor_hist.append(factors.copy())\n",
    "\n",
    "        factors_test = factors.copy()\n",
    "        factors_test[i] *= 1.1\n",
    "\n",
    "        arg_list = [n_games, model, tmax, factors_mid_to_long(factors), \n",
    "                                        factors_mid_to_long(factors_test)]\n",
    "\n",
    "        process_list.append(multiprocessing.Process(target=run_test_games, \n",
    "                                           args=(\"Process 1\", arg_list, output_queue)))\n",
    "\n",
    "    \n",
    "    print('start processes')\n",
    "\n",
    "    for process in process_list:\n",
    "        process.start()\n",
    "\n",
    "    print('wait for processes')\n",
    "    \n",
    "    for process in process_list:\n",
    "        process.join()\n",
    "    \n",
    "    print('evaluate results')\n",
    "\n",
    "    results = []\n",
    "    while not output_queue.empty():\n",
    "        results.extend(output_queue.get())\n",
    "\n",
    "    for i in range(6):\n",
    "\n",
    "        wins_1, draws, wins_2 = results[i]\n",
    "\n",
    "        if wins_1 > wins_2:\n",
    "            factors[i] *= 1/1.1\n",
    "        elif wins_2 > wins_1:\n",
    "            factors[i] *= 1.1\n",
    "\n",
    "    factors = 1/np.linalg.norm(factors) * factors \n",
    "\n",
    "    print('duration: ', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i[0] for i in factor_hist])\n",
    "plt.plot([i[1] for i in factor_hist])\n",
    "plt.plot([i[2] for i in factor_hist])\n",
    "plt.plot([i[3] for i in factor_hist])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n",
      "tensor(-14.8880, grad_fn=<SumBackward0>)\n",
      "Jacobian shape: torch.Size([10, 12, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd.functional as F\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(12 * 8 * 8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        return self.fc(x_flat)\n",
    "\n",
    "\n",
    "# Create an instance of the model\n",
    "test_model = SimpleModel()\n",
    "\n",
    "# Generate some input data (vectorized version)\n",
    "batch_size = 10\n",
    "vectorized_input = torch.ones((batch_size, 12, 8, 8), requires_grad=True).view(batch_size, -1)\n",
    "jacobian_matrix = F.jacobian(lambda x: model(x.view(-1, 12, 8, 8)).sum(), vectorized_input)\n",
    "jacobian = jacobian_matrix.view(batch_size, 12, 8, 8)\n",
    "\n",
    "print(\"Jacobian shape:\", jacobian.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.2408, -0.2951, -0.0612, -0.0810]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.4001], requires_grad=True)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in test_model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
